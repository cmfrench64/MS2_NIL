{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Selenium & Beatuiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully recovered 326 athletes for basketball 2023!\n",
      "========== Starting Scrape ===========\n",
      "=== Scraped 3.07 % of the Players ===\n",
      "326 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      "======================================\n",
      "=== Scraped 6.13 % of the Players ===\n",
      "326 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21\n",
      "======================================\n",
      "=== Scraped 9.20 % of the Players ===\n",
      "326 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31\n",
      "======================================\n",
      "=== Scraped 12.27 % of the Players ===\n",
      "326 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      "======================================\n",
      "=== Scraped 15.34 % of the Players ===\n",
      "326 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51\n",
      "======================================\n",
      "=== Scraped 18.40 % of the Players ===\n",
      "326 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61\n",
      "======================================\n",
      "=== Scraped 21.47 % of the Players ===\n",
      "326 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71\n",
      "======================================\n",
      "=== Scraped 24.54 % of the Players ===\n",
      "326 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81\n",
      "======================================\n",
      "=== Scraped 27.61 % of the Players ===\n",
      "326 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91\n",
      "======================================\n",
      "=== Scraped 30.67 % of the Players ===\n",
      "326 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n",
      "======================================\n",
      "=== Scraped 33.74 % of the Players ===\n",
      "326 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111\n",
      "======================================\n",
      "=== Scraped 36.81 % of the Players ===\n",
      "326 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      "======================================\n",
      "=== Scraped 39.88 % of the Players ===\n",
      "326 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131\n",
      "======================================\n",
      "=== Scraped 42.94 % of the Players ===\n",
      "326 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141\n",
      "======================================\n",
      "=== Scraped 46.01 % of the Players ===\n",
      "326 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151\n",
      "======================================\n",
      "Error on the 'Recruiting' page for athlete: Aaron McBride\n",
      "Removing all previous information about this player! (i.e name, exp, athlete_grade, ages, ranks, high_school, home_town, pos_height_weight)\n",
      "=== Scraped 49.08 % of the Players ===\n",
      "325 160 159 160 160 161 160 160 160 160 160 160 160 160 160 160\n",
      "======================================\n",
      "Error on the 'Player' page for athlete: Jayden Reid\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Bryce Lindsay\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Brooklyn Hicks\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jordan Burks\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Brock Harding\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Bron Roberts\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Marquavious Brown\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Robert Davis\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Rahmir Barno\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Kendrick De Luna\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Xavier Edmonds\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jarred Hall\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Owen Freeman\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Justin Johnson\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Donovan Raymond\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jason Fontenet Jr.\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Brock Vice\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Isaiah Watts\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Miles Rubin\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Kayden Fish\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Isaiah West\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: R.J. Johnson\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Logan Imes\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Gabe Warren\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Asa Thomas\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Tristan Gross\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Thomas Haugh\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Fred Payne\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Connor Dubsky\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Avantae Parker\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Braeden Shrewsberry\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Mikey Price\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Tyler Ringgold\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jayden Lemond\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Xzayvier Brown\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Lawrent Rice\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Joey Hart\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jimmy Oladokun\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Christian King\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Bangot Dak\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jaylen Crocker-Johnson\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: John Blackwell\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Malik Olafioye\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jalen Hooks\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Chuck Bailey III\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Isaiah Manning\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Trevor Smith\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Dalen Davis\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Zyier Beverly\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Desmond White\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: James Johns Jr.\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: John Gamble\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Mier Panoam\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Ryan Forrest\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Zacharie Perrin\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Sultan Adewale\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Spencer Elliott\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Braden Pierce\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Cian Medley\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: DK Manyiel\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Davius Loury\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Connor Dow\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Alpha Chibambe\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Josiah Dotzler\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Trey Autry\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Blake Barkley\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Adrian Meyers\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Sterling Knox\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Boden Kapke\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Joey Brown\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jordann Dumont\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jayden Ndjigue\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Zion Stanford\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Petras Padegimas\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: William Patterson\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Royal Brown\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Kenneth Lewis\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jerry Deng\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Christian Jones\n",
      "Removing all previous information about this player! (i.e name)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     driver\u001b[39m.\u001b[39;49mget(athlete_link)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/mvizzini951/MS2_NIL/venv/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:353\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m: url})\n",
      "File \u001b[0;32m~/Documents/mvizzini951/MS2_NIL/venv/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m--> 344\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[1;32m    345\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/Documents/mvizzini951/MS2_NIL/venv/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=117.0.5938.88)\nStacktrace:\n0   chromedriver                        0x000000010247ad98 chromedriver + 4337048\n1   chromedriver                        0x0000000102472e14 chromedriver + 4304404\n2   chromedriver                        0x000000010209fa5c chromedriver + 293468\n3   chromedriver                        0x00000001020786c0 chromedriver + 132800\n4   chromedriver                        0x000000010210c04c chromedriver + 737356\n5   chromedriver                        0x000000010211f198 chromedriver + 815512\n6   chromedriver                        0x00000001020d8a5c chromedriver + 526940\n7   chromedriver                        0x00000001020d9908 chromedriver + 530696\n8   chromedriver                        0x0000000102440de4 chromedriver + 4099556\n9   chromedriver                        0x00000001024452a0 chromedriver + 4117152\n10  chromedriver                        0x000000010244b52c chromedriver + 4142380\n11  chromedriver                        0x0000000102445da0 chromedriver + 4119968\n12  chromedriver                        0x000000010241da74 chromedriver + 3955316\n13  chromedriver                        0x0000000102462a48 chromedriver + 4237896\n14  chromedriver                        0x0000000102462bc4 chromedriver + 4238276\n15  chromedriver                        0x0000000102472a8c chromedriver + 4303500\n16  libsystem_pthread.dylib             0x00000001951cbfa8 _pthread_start + 148\n17  libsystem_pthread.dylib             0x00000001951c6da0 thread_start + 8\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m             pos_height_weight\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mnan)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError on the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mPlayer\u001b[39m\u001b[39m'\u001b[39m\u001b[39m page for athlete: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(names_[i]))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRemoving all previous information about this player! (i.e name)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m     names\u001b[39m.\u001b[39mpop(i)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sports = ['basketball']\n",
    "#years = ['2022','2023','2024','2025','2026']\n",
    "years = ['2023']\n",
    "\n",
    "exp = []\n",
    "athlete_grade = []\n",
    "pos_height_weight = []\n",
    "ages = []\n",
    "ranks = []\n",
    "high_school = []\n",
    "home_town = []\n",
    "colleges = []\n",
    "college_status = []\n",
    "college_distance = []\n",
    "num_offers = []\n",
    "NIL_val = []\n",
    "instagram_followers = []\n",
    "twitter_followers = []\n",
    "tiktok_followers = []\n",
    "\n",
    "for year in years:\n",
    "    for sport in sports:\n",
    "        \n",
    "        # Selenium Driver to click on \"Load More\"\n",
    "        driver = webdriver.Chrome()\n",
    "        URL = f'https://www.on3.com/db/rankings/industry-player/{sport}/{year}/'\n",
    "        driver.get(URL)\n",
    "\n",
    "        dummyCount = 0\n",
    "        \n",
    "        ###########################################################################################################################\n",
    "        # Click the \"Load More\" button such that all athelete links are visible\n",
    "        # dummyCount was implemented for sports/years in which the number of athletes is very big (ex: football 2023 has 3000+ athletes) AND\n",
    "        # the 'Load More' button does not disappear when pressed to completiion... so a simple counter was implemented\n",
    "        # By making this condition: 'dummyCount < 19' we max out at 1000 athletes\n",
    "        while (dummyCount < 19):\n",
    "            try:\n",
    "                load_more_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, \"//span[@class='MuiButton-label' and contains(text(), 'Load More')]\"))\n",
    "                )\n",
    "                load_more_button.click()\n",
    "                time.sleep(10)\n",
    "                dummyCount += 1\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        ###########################################################################################################################\n",
    "        \n",
    "        ###########################################################################################################################\n",
    "        # Get all athlete names & associated links\n",
    "        # New Information: names, links\n",
    "        page_source = driver.page_source\n",
    "        page_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        results = page_soup.find(class_=\"PlayerRankings_playerRankings__jvfFg\")\n",
    "\n",
    "        athletes = results.find_all('a', class_=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-h5 MuiTypography-colorPrimary\")\n",
    "        names = [athlete.text for athlete in athletes]\n",
    "        names_ = names\n",
    "\n",
    "        # Generate a list of links that we can iterate through\n",
    "        links = [athlete['href'] for athlete in athletes]\n",
    "        base_url = \"https://www.on3.com/\"\n",
    "        athlete_links = [urljoin(base_url, link) for link in links]\n",
    "\n",
    "        tot = len(athlete_links)\n",
    "\n",
    "        print('Successfully recovered {} athletes for {} {}!'.format(tot, sport, year))\n",
    "        print('========== Starting Scrape ===========')\n",
    "\n",
    "        ###########################################################################################################################\n",
    "\n",
    "        for i, athlete_link in enumerate(athlete_links):\n",
    "\n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"Player\" page for each of the athletes\n",
    "            # New Information: exp, athlete_grade, ages, high_school, home_town, pos_height_weight\n",
    "            try:\n",
    "                driver.get(athlete_link)\n",
    "                time.sleep(3)\n",
    "\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            except:\n",
    "                print(\"Player page not loading for athlete {}\".format(names_[i]))\n",
    "                exp.append(np.nan)\n",
    "                athlete_grade.append(np.nan)\n",
    "                pos_height_weight.append(np.nan)\n",
    "                ages.append(np.nan)\n",
    "                ranks.append(np.nan)\n",
    "                high_school.append(np.nan)\n",
    "                home_town.append(np.nan)\n",
    "                colleges.append(np.nan)\n",
    "                college_status.append(np.nan)\n",
    "                college_distance.append(np.nan)\n",
    "                num_offers.append(np.nan)\n",
    "                NIL_val.append(np.nan)\n",
    "                instagram_followers.append(np.nan)\n",
    "                twitter_followers.append(np.nan)\n",
    "                tiktok_followers.append(np.nan)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                CollegeRankingInfo = page_soup.find(class_='CollegeRanking_info__LM3nn')\n",
    "\n",
    "                if CollegeRankingInfo:\n",
    "                    exp_year = CollegeRankingInfo.find_all(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    if len(exp_year) == 2:\n",
    "                        exp.append(exp_year[0].text)\n",
    "                        athlete_grade.append(exp_year[1].text)\n",
    "                    else:\n",
    "                        exp.append(np.nan)\n",
    "                        athlete_grade.append(np.nan)\n",
    "                    age = CollegeRankingInfo.find(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorPrimary')\n",
    "                    ages.append(age.text)\n",
    "                else:\n",
    "                    exp.append(np.nan)\n",
    "                    athlete_grade.append(np.nan)\n",
    "                    ages.append(np.nan)\n",
    "            except:\n",
    "                print(\"exp, athlete_grade, age not available for athlete {}\".format(names_[i]))\n",
    "                exp.append(np.nan)\n",
    "                athlete_grade.append(np.nan)\n",
    "                ages.append(np.nan)\n",
    "            \n",
    "            try:\n",
    "                RecruitModuleInfo = page_soup.find(class_='RecruitModule_info__Ugxqd')\n",
    "\n",
    "                if RecruitModuleInfo:\n",
    "                    homeInfo = RecruitModuleInfo.find_all(class_='MuiTypography-root RecruitModule_span__KmmzN MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    high_school.append(homeInfo[-2].text)\n",
    "                    home_town.append(homeInfo[-1].text)\n",
    "                else:\n",
    "                    high_school.append(np.nan)\n",
    "                    home_town.append(np.nan)\n",
    "            except:\n",
    "                print(\"high_school, home_town not available for athlete {}\".format(names_[i]))\n",
    "                high_school.append(np.nan)\n",
    "                home_town.append(np.nan)\n",
    "\n",
    "            try:\n",
    "                Attributes = page_soup.find(class_='MeasurementInfo_info__IHmGD')\n",
    "\n",
    "                if Attributes:\n",
    "                    dummy = Attributes.find_all(class_='MuiTypography-root MeasurementInfo_text__dCryI MuiTypography-body1 MuiTypography-colorTextPrimary')\n",
    "                    if len(dummy) >= 2:\n",
    "                        pos_height_weight.append(dummy[1].text)\n",
    "                    else:\n",
    "                        pos_height_weight.append(np.nan)\n",
    "            except:\n",
    "                print(\"pos_height_weight not available for athlete {}\".format(names_[i]))\n",
    "                pos_height_weight.append(np.nan)\n",
    "            ###########################################################################################################################\n",
    "            \n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"Recruiting\" page for each of the athletes\n",
    "            # New Information: ranks, colleges (the college this athlete is targeting), college_status, college_distance, num_offers\n",
    "            try:\n",
    "                driver.get(urljoin(athlete_link,'recruiting/'))\n",
    "                time.sleep(3)\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "\n",
    "            except:\n",
    "                print(\"Recruiting page not loading for athlete {}\".format(names_[i]))\n",
    "                ranks.append(np.nan)\n",
    "                colleges.append(np.nan)\n",
    "                college_status.append(np.nan)\n",
    "                college_distance.append(np.nan)\n",
    "                num_offers.append(np.nan)\n",
    "                NIL_val.append(np.nan)\n",
    "                instagram_followers.append(np.nan)\n",
    "                twitter_followers.append(np.nan)\n",
    "                tiktok_followers.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                Ranks = page_soup.find(class_=\"Rankings_industryRankWrapper__2qwnq\")\n",
    "\n",
    "                if Ranks:\n",
    "                    ranking = Ranks.find(class_=\"MuiTypography-root Rankings_industryRating__9uavm MuiTypography-body1 MuiTypography-colorTextPrimary\")\n",
    "                    ranks.append(ranking.text)\n",
    "                else:\n",
    "                    ranks.append(np.nan)\n",
    "            except:\n",
    "                print(\"ranks not available for athlete {}\".format(names_[i]))\n",
    "                ranks.append(np.nan)\n",
    "\n",
    "            try:\n",
    "                # Find url for all teams and navigate to page\n",
    "                all_team_link = page_soup.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineHover PlayerInterestsModule_text__kjqNU MuiTypography-caption MuiTypography-colorPrimary')\n",
    "                driver.get(urljoin(base_url,all_team_link['href']))\n",
    "                time.sleep(3)\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            except:\n",
    "                print(\"all_team_link page is not loading for athlete {}\".format(names_[i]))\n",
    "                colleges.append(np.nan)\n",
    "                college_status.append(np.nan)\n",
    "                college_distance.append(np.nan)\n",
    "                num_offers.append(np.nan)\n",
    "                NIL_val.append(np.nan)\n",
    "                instagram_followers.append(np.nan)\n",
    "                twitter_followers.append(np.nan)\n",
    "                tiktok_followers.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "\n",
    "                RecruitColleges = page_soup.find_all(class_='PlayerInterestsItem_teamContainer__vjQkf')\n",
    "\n",
    "                if RecruitColleges:\n",
    "                    dummy_colleges=[]\n",
    "                    count = 0\n",
    "                \n",
    "                    for college in RecruitColleges:\n",
    "                        college_name = college.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineNone PlayerInterestsItem_teamName__FeBHv MuiTypography-h5 MuiTypography-colorPrimary')\n",
    "\n",
    "                        if year == '2024' or year == '2025' or year == '2026':\n",
    "                            college_statuses = college.find(class_='MuiTypography-root PlayerInterestsItem_status__1_rA8 PlayerInterestsItem_offered__OxPV0 MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                        else:\n",
    "                            college_statuses = college.find(class_='MuiTypography-root PlayerInterestsItem_status__1_rA8 MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                        \n",
    "                        college_dist = college.find(class_='MuiTypography-root PlayerInterestsItem_distanceText__KJhj3 MuiTypography-caption MuiTypography-colorTextPrimary')\n",
    "                        if count == 0:\n",
    "                            colleges.append(college_name.text)\n",
    "                            college_status.append(college_statuses.text)\n",
    "                            college_distance.append(college_dist.text)\n",
    "                        count +=1\n",
    "                    num_offers.append(count)\n",
    "                else:\n",
    "                    colleges.append(np.nan)\n",
    "                    college_status.append(np.nan)\n",
    "                    college_distance.append(np.nan)\n",
    "                    num_offers.append(np.nan)\n",
    "            except:\n",
    "                print(\"colleges, college_status, college_distance, num_offers are not available for athlete {}\".format(names_[i]))\n",
    "                colleges.append(np.nan)\n",
    "                college_status.append(np.nan)\n",
    "                college_distance.append(np.nan)\n",
    "                num_offers.append(np.nan)\n",
    "            ###########################################################################################################################\n",
    "            \n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"NIL\" page for each of the athletes\n",
    "            # New Information: NIL_val, instagram_followers, twitter_followers, tiktok_followers\n",
    "            try:\n",
    "                driver.get(urljoin(athlete_link,'nil/'))\n",
    "                time.sleep(3)\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            except:\n",
    "                print(\"NIL page not loading for athlete {}\".format(names_[i]))\n",
    "                NIL_val.append(np.nan)\n",
    "                instagram_followers.append(np.nan)\n",
    "                twitter_followers.append(np.nan)\n",
    "                tiktok_followers.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                RecruitNIL = page_soup.find(class_='NilValuationCircle_nilCircleValue__wzomB')\n",
    "\n",
    "                if RecruitNIL:\n",
    "                    NIL_val.append(RecruitNIL.text)\n",
    "                else:\n",
    "                    NIL_val.append(np.nan)\n",
    "            except:\n",
    "                print(\"NIL_val not available for athlete {}\".format(names_[i]))\n",
    "                NIL_val.append(np.nan)\n",
    "            \n",
    "            try:\n",
    "                RecruitSocials = page_soup.find(class_=\"NilSocialValuations_socialValuations__MeR7O\")\n",
    "\n",
    "                instagram = np.nan\n",
    "                twitter = np.nan\n",
    "                tiktok = np.nan\n",
    "\n",
    "                if RecruitSocials:\n",
    "                    socials = RecruitSocials.find_all(class_=\"NilSocialValuations_platform__3qBMy\")\n",
    "                    for social in socials:\n",
    "                        social_name = social.find('a')['href']\n",
    "                        if 'instagram' in social_name:\n",
    "                            instagram = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                        elif 'twitter' in social_name:\n",
    "                            twitter = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                        elif 'tiktok' in social_name:\n",
    "                            tiktok = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                \n",
    "                instagram_followers.append(instagram)\n",
    "                twitter_followers.append(twitter)\n",
    "                tiktok_followers.append(tiktok)\n",
    "            \n",
    "            except:\n",
    "                print(\"insta, twitter, and tiktok followers not available for athlete {}\".format(names_[i]))\n",
    "                instagram_followers.append(np.nan)\n",
    "                twitter_followers.append(np.nan)\n",
    "                tiktok_followers.append(np.nan)\n",
    "            ###########################################################################################################################\n",
    "\n",
    "            ###########################################################################################################################\n",
    "            # Print something out to the user, like a progress bar\n",
    "            if (i % 10 == 0) and (i != 0):\n",
    "                print(\"=== Scraped {:.2f} % of the Players ===\".format((i/tot)*100))\n",
    "                print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "                      len(colleges), len(college_status), len(college_distance), len(num_offers), len(NIL_val),\n",
    "                      len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "                print(\"======================================\")\n",
    "            ###########################################################################################################################\n",
    "        \n",
    "        print(\"=== Scraped 100.0 % of the Players ===\")\n",
    "        print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "              len(colleges), len(college_status), len(college_distance), len(num_offers), len(NIL_val),\n",
    "              len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "        print(\"======================================\")\n",
    "        print('Successfully completed {} athletes for {} {}!'.format(len(names), sport, year))\n",
    "        print('Number of removed athletes: {}'.format(len(athlete_links) - len(names)))\n",
    "        #end\n",
    "    #end\n",
    "#end\n",
    "\n",
    "# Close the instance of the webpage\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n"
     ]
    }
   ],
   "source": [
    "print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "      len(colleges), len(college_status), len(college_distance), len(num_offers), len(NIL_val),\n",
    "      len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "# print(names[0], exp[0], pos_height_weight[0], athlete_grade[0], ages[0], ranks[0], high_school[0])\n",
    "# print(home_town[0], colleges[0], college_status[0], college_distance[0], num_offers[0])\n",
    "# print(NIL_val[0], instagram_followers[0], twitter_followers[0], tiktok_followers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv saved to 'csv_files' folder!'\n"
     ]
    }
   ],
   "source": [
    "column_names = ['NAME', 'EXP', 'POS_HEI_WEI', 'GRADE', 'AGE', 'SKILL', 'HISCH', 'HOTOWN', 'STARCOLL', 'STARCOLLSTAT', 'COLLDIST', 'NUMOFF', 'INSTA', 'TWIT', 'TIK', 'NILVAL']\n",
    "\n",
    "csv_file_path = 'csv_files/{}_{}.csv'.format(sport, year)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "    writer.writerow(column_names)\n",
    "\n",
    "    for row in zip(names, exp, pos_height_weight, athlete_grade, ages, ranks, high_school, home_town, colleges, college_status, college_distance, num_offers, instagram_followers, twitter_followers, tiktok_followers, NIL_val):\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"csv saved to 'csv_files' folder!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
