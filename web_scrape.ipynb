{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Selenium & Beatuiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully recovered 45 athletes for basketball 2026!\n",
      "========== Starting Scrape ===========\n",
      "all_team_link page is not loading for athlete Caleb Gaskins\n",
      "all_team_link page is not loading for athlete Jason Crowe\n",
      "=== Scraped 22.22 % of the Players ===\n",
      "45 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      "======================================\n",
      "all_team_link page is not loading for athlete Cam Holmes\n",
      "all_team_link page is not loading for athlete Dylan Mingo\n",
      "all_team_link page is not loading for athlete Maximo Adams\n",
      "=== Scraped 44.44 % of the Players ===\n",
      "45 21 21 21 21 21 21 21 21 21 21 21 21 21 21\n",
      "======================================\n",
      "all_team_link page is not loading for athlete Trent Perry\n",
      "all_team_link page is not loading for athlete TJ Crumble\n",
      "all_team_link page is not loading for athlete Dhani Miller\n",
      "all_team_link page is not loading for athlete Chris Washington\n",
      "=== Scraped 66.67 % of the Players ===\n",
      "45 31 31 31 31 31 31 31 31 31 31 31 31 31 31\n",
      "======================================\n",
      "all_team_link page is not loading for athlete Jayden Hodge\n",
      "all_team_link page is not loading for athlete Keiner Asprilla\n",
      "all_team_link page is not loading for athlete Toni Bryant\n",
      "all_team_link page is not loading for athlete Ben Ahmed\n",
      "=== Scraped 88.89 % of the Players ===\n",
      "45 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      "======================================\n",
      "=== Scraped 100.0 % of the Players ===\n",
      "45 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n",
      "======================================\n",
      "Successfully completed 45 athletes for basketball 2026!\n"
     ]
    }
   ],
   "source": [
    "sports = ['basketball']\n",
    "#years = ['2022','2023','2024','2025','2026']\n",
    "years = ['2026']\n",
    "\n",
    "exp = []\n",
    "athlete_grade = []\n",
    "pos_height_weight = []\n",
    "ages = []\n",
    "ranks = []\n",
    "high_school = []\n",
    "home_town = []\n",
    "colleges = []\n",
    "college_distance = []\n",
    "num_offers = []\n",
    "NIL_val = []\n",
    "instagram_followers = []\n",
    "twitter_followers = []\n",
    "tiktok_followers = []\n",
    "\n",
    "for year in years:\n",
    "    for sport in sports:\n",
    "        # Selenium Driver to click on \"Load More\"\n",
    "        driver = webdriver.Chrome()\n",
    "        URL = f'https://www.on3.com/db/rankings/industry-player/{sport}/{year}/'\n",
    "        driver.get(URL)\n",
    "\n",
    "        dummyCount = 0\n",
    "        ###########################################################################################################################\n",
    "        # Click the \"Load More\" button such that all athelete links are visible\n",
    "        # dummyCount was implemented for sports/years in which the number of athletes is very big (ex: football 2023 has 3000+ athletes) AND\n",
    "        # the 'Load More' button does not disappear when pressed to completiion... so a simple counter was implemented\n",
    "        # By making this condition: 'dummyCount < 19' we max out at 1000 athletes\n",
    "        while (dummyCount < 60):\n",
    "            try:\n",
    "                load_more_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, \"//span[@class='MuiButton-label' and contains(text(), 'Load More')]\"))\n",
    "                )\n",
    "                load_more_button.click()\n",
    "                time.sleep(12)\n",
    "                dummyCount += 1\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        ###########################################################################################################################\n",
    "        \n",
    "        ###########################################################################################################################\n",
    "        # Get all athlete names & associated links\n",
    "        # New Information: names, links\n",
    "        page_source = driver.page_source\n",
    "        page_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        results = page_soup.find(class_=\"PlayerRankings_playerRankings__jvfFg\")\n",
    "\n",
    "        athletes = results.find_all('a', class_=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-h5 MuiTypography-colorPrimary\")\n",
    "        names = [athlete.text for athlete in athletes]\n",
    "        names_ = names\n",
    "\n",
    "        # Generate a list of links that we can iterate through\n",
    "        links = [athlete['href'] for athlete in athletes]\n",
    "        base_url = \"https://www.on3.com/\"\n",
    "        athlete_links = [urljoin(base_url, link) for link in links]\n",
    "\n",
    "        tot = len(athlete_links)\n",
    "\n",
    "        print('Successfully recovered {} athletes for {} {}!'.format(tot, sport, year))\n",
    "        print('========== Starting Scrape ===========')\n",
    "\n",
    "        ###########################################################################################################################\n",
    "\n",
    "        for i, athlete_link in enumerate(athlete_links):\n",
    "\n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"Player\" page for each of the athletes\n",
    "            # New Information: exp, athlete_grade, ages, high_school, home_town, pos_height_weight\n",
    "            try:\n",
    "                driver.get(athlete_link)\n",
    "                time.sleep(3)\n",
    "\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            except:\n",
    "                print(\"Player page not loading for athlete {}\".format(names_[i]))\n",
    "                exp.append(np.nan)\n",
    "                athlete_grade.append(np.nan)\n",
    "                pos_height_weight.append(np.nan)\n",
    "                ages.append(np.nan)\n",
    "                ranks.append(np.nan)\n",
    "                high_school.append(np.nan)\n",
    "                home_town.append(np.nan)\n",
    "                colleges.append(np.nan)\n",
    "                college_distance.append(np.nan)\n",
    "                num_offers.append(np.nan)\n",
    "                NIL_val.append(np.nan)\n",
    "                instagram_followers.append(np.nan)\n",
    "                twitter_followers.append(np.nan)\n",
    "                tiktok_followers.append(np.nan)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                CollegeRankingInfo = page_soup.find(class_='CollegeRanking_info__LM3nn')\n",
    "\n",
    "                if CollegeRankingInfo:\n",
    "                    exp_year = CollegeRankingInfo.find_all(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    if len(exp_year) == 2:\n",
    "                        exp.append(exp_year[0].text)\n",
    "                        athlete_grade.append(exp_year[1].text)\n",
    "                    else:\n",
    "                        exp.append(np.nan)\n",
    "                        athlete_grade.append(np.nan)\n",
    "                    age = CollegeRankingInfo.find(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorPrimary')\n",
    "                    ages.append(age.text)\n",
    "                else:\n",
    "                    exp.append(np.nan)\n",
    "                    athlete_grade.append(np.nan)\n",
    "                    ages.append(np.nan)\n",
    "            except:\n",
    "                print(\"exp, athlete_grade, age not available for athlete {}\".format(names_[i]))\n",
    "                exp.append(np.nan)\n",
    "                athlete_grade.append(np.nan)\n",
    "                ages.append(np.nan)\n",
    "            \n",
    "            try:\n",
    "                RecruitModuleInfo = page_soup.find(class_='RecruitModule_info__Ugxqd')\n",
    "\n",
    "                if RecruitModuleInfo:\n",
    "                    homeInfo = RecruitModuleInfo.find_all(class_='MuiTypography-root RecruitModule_span__KmmzN MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    high_school.append(homeInfo[-2].text)\n",
    "                    home_town.append(homeInfo[-1].text)\n",
    "                else:\n",
    "                    high_school.append(np.nan)\n",
    "                    home_town.append(np.nan)\n",
    "            except:\n",
    "                print(\"high_school, home_town not available for athlete {}\".format(names_[i]))\n",
    "                high_school.append(np.nan)\n",
    "                home_town.append(np.nan)\n",
    "\n",
    "            try:\n",
    "                Attributes = page_soup.find(class_='MeasurementInfo_info__IHmGD')\n",
    "\n",
    "                if Attributes:\n",
    "                    dummy = Attributes.find_all(class_='MuiTypography-root MeasurementInfo_text__dCryI MuiTypography-body1 MuiTypography-colorTextPrimary')\n",
    "                    pos_height_weight.append(dummy[1].text)\n",
    "                else:\n",
    "                    pos_height_weight.append(np.nan)\n",
    "            except:\n",
    "                print(\"pos_height_weight not available for athlete {}\".format(names_[i]))\n",
    "                pos_height_weight.append(np.nan)\n",
    "            ###########################################################################################################################\n",
    "            \n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"Recruiting\" page for each of the athletes\n",
    "            # New Information: ranks, colleges (the college this athlete is targeting), college_distance, num_offers\n",
    "            try:\n",
    "                driver.get(urljoin(athlete_link,'recruiting/'))\n",
    "                time.sleep(3)\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "\n",
    "            except:\n",
    "                print(\"Recruiting page not loading for athlete {}\".format(names_[i]))\n",
    "                ranks.append(np.nan)\n",
    "                colleges.append(np.nan)\n",
    "                college_distance.append(np.nan)\n",
    "                num_offers.append(np.nan)\n",
    "                NIL_val.append(np.nan)\n",
    "                instagram_followers.append(np.nan)\n",
    "                twitter_followers.append(np.nan)\n",
    "                tiktok_followers.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                Ranks = page_soup.find(class_=\"Rankings_industryRankWrapper__2qwnq\")\n",
    "\n",
    "                if Ranks:\n",
    "                    ranking = Ranks.find(class_=\"MuiTypography-root Rankings_industryRating__9uavm MuiTypography-body1 MuiTypography-colorTextPrimary\")\n",
    "                    ranks.append(ranking.text)\n",
    "                else:\n",
    "                    ranks.append(np.nan)\n",
    "            except:\n",
    "                print(\"ranks not available for athlete {}\".format(names_[i]))\n",
    "                ranks.append(np.nan)\n",
    "\n",
    "            try:\n",
    "                # Find url for all teams and navigate to page\n",
    "                all_team_link = page_soup.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineHover PlayerInterestsModule_text__kjqNU MuiTypography-caption MuiTypography-colorPrimary')\n",
    "                driver.get(urljoin(base_url,all_team_link['href']))\n",
    "                time.sleep(5)\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            except:\n",
    "                print(\"all_team_link page is not loading for athlete {}\".format(names_[i]))\n",
    "                colleges.append(np.nan)\n",
    "                college_distance.append(np.nan)\n",
    "                num_offers.append(np.nan)\n",
    "                NIL_val.append(np.nan)\n",
    "                instagram_followers.append(np.nan)\n",
    "                twitter_followers.append(np.nan)\n",
    "                tiktok_followers.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "\n",
    "                RecruitColleges = page_soup.find_all(class_='PlayerInterestsItem_teamContainer__vjQkf')\n",
    "\n",
    "                if RecruitColleges:\n",
    "                    count = 0\n",
    "                    for college in RecruitColleges:\n",
    "                        college_name = college.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineNone PlayerInterestsItem_teamName__FeBHv MuiTypography-h5 MuiTypography-colorPrimary')\n",
    "                        college_dist = college.find(class_='MuiTypography-root PlayerInterestsItem_distanceText__KJhj3 MuiTypography-caption MuiTypography-colorTextPrimary')\n",
    "\n",
    "                        if count == 0:\n",
    "                            colleges.append(college_name.text)\n",
    "                            college_distance.append(college_dist.text)\n",
    "                        count +=1\n",
    "                    num_offers.append(count)\n",
    "                else:\n",
    "                    colleges.append(np.nan)\n",
    "                    college_distance.append(np.nan)\n",
    "                    num_offers.append(np.nan)\n",
    "            except:\n",
    "                print(\"colleges, college_distance, num_offers are not available for athlete {}\".format(names_[i]))\n",
    "                colleges.append(np.nan)\n",
    "                college_distance.append(np.nan)\n",
    "                num_offers.append(np.nan)\n",
    "            ###########################################################################################################################\n",
    "            \n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"NIL\" page for each of the athletes\n",
    "            # New Information: NIL_val, instagram_followers, twitter_followers, tiktok_followers\n",
    "            try:\n",
    "                driver.get(urljoin(athlete_link,'nil/'))\n",
    "                time.sleep(3)\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            except:\n",
    "                print(\"NIL page not loading for athlete {}\".format(names_[i]))\n",
    "                NIL_val.append(np.nan)\n",
    "                instagram_followers.append(np.nan)\n",
    "                twitter_followers.append(np.nan)\n",
    "                tiktok_followers.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                RecruitNIL = page_soup.find(class_='NilValuationCircle_nilCircleValue__wzomB')\n",
    "\n",
    "                if RecruitNIL:\n",
    "                    NIL_val.append(RecruitNIL.text)\n",
    "                else:\n",
    "                    NIL_val.append(np.nan)\n",
    "            except:\n",
    "                print(\"NIL_val not available for athlete {}\".format(names_[i]))\n",
    "                NIL_val.append(np.nan)\n",
    "            \n",
    "            try:\n",
    "                RecruitSocials = page_soup.find(class_=\"NilSocialValuations_socialValuations__MeR7O\")\n",
    "\n",
    "                instagram = np.nan\n",
    "                twitter = np.nan\n",
    "                tiktok = np.nan\n",
    "\n",
    "                if RecruitSocials:\n",
    "                    socials = RecruitSocials.find_all(class_=\"NilSocialValuations_platform__3qBMy\")\n",
    "                    for social in socials:\n",
    "                        social_name = social.find('a')['href']\n",
    "                        if 'instagram' in social_name:\n",
    "                            instagram = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                        elif 'twitter' in social_name:\n",
    "                            twitter = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                        elif 'tiktok' in social_name:\n",
    "                            tiktok = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                \n",
    "                instagram_followers.append(instagram)\n",
    "                twitter_followers.append(twitter)\n",
    "                tiktok_followers.append(tiktok)\n",
    "            \n",
    "            except:\n",
    "                print(\"insta, twitter, and tiktok followers not available for athlete {}\".format(names_[i]))\n",
    "                instagram_followers.append(np.nan)\n",
    "                twitter_followers.append(np.nan)\n",
    "                tiktok_followers.append(np.nan)\n",
    "            ###########################################################################################################################\n",
    "\n",
    "            ###########################################################################################################################\n",
    "            # Print something out to the user, like a progress bar\n",
    "            if (i % 10 == 0) and (i != 0):\n",
    "                print(\"=== Scraped {:.2f} % of the Players ===\".format((i/tot)*100))\n",
    "                print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "                      len(colleges), len(college_distance), len(num_offers), len(NIL_val),\n",
    "                      len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "                print(\"======================================\")\n",
    "            ###########################################################################################################################\n",
    "        \n",
    "        print(\"=== Scraped 100.0 % of the Players ===\")\n",
    "        print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "              len(colleges), len(college_distance), len(num_offers), len(NIL_val),\n",
    "              len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "        print(\"======================================\")\n",
    "        print('Successfully completed {} athletes for {} {}!'.format(len(names), sport, year))\n",
    "        #end\n",
    "    #end\n",
    "#end\n",
    "\n",
    "# Close the instance of the webpage\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n"
     ]
    }
   ],
   "source": [
    "print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "      len(colleges), len(college_distance), len(num_offers), len(NIL_val),\n",
    "      len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "# print(names[0], exp[0], pos_height_weight[0], athlete_grade[0], ages[0], ranks[0], high_school[0])\n",
    "# print(home_town[0], colleges[0], college_distance[0], num_offers[0])\n",
    "# print(NIL_val[0], instagram_followers[0], twitter_followers[0], tiktok_followers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv saved to 'csv_files' folder!'\n"
     ]
    }
   ],
   "source": [
    "column_names = ['NAME', 'EXP', 'POS_HEI_WEI', 'GRADE', 'AGE', 'SKILL', 'HISCH', 'HOTOWN', 'STARCOLL', 'COLLDIST', 'NUMOFF', 'INSTA', 'TWIT', 'TIK', 'NILVAL']\n",
    "\n",
    "csv_file_path = 'csv_files/{}_{}.csv'.format(sport, year)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "    writer.writerow(column_names)\n",
    "\n",
    "    for row in zip(names, exp, pos_height_weight, athlete_grade, ages, ranks, high_school, home_town, colleges, college_distance, num_offers, instagram_followers, twitter_followers, tiktok_followers, NIL_val):\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"csv saved to 'csv_files' folder!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
