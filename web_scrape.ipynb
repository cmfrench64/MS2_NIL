{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Attempt just using Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = ['college', 'high-school']\n",
    "sports = ['football', 'basketball', 'baseball', 'womens-basketball', 'volleyball', 'gymnastics',\n",
    "         'mens-lacrosse', 'womens-lacrosse', 'mens-soccer', 'womens-soccer', 'softball', 'womens-track',\n",
    "         'mens-golf', 'womens-golf', 'mens-hockey', 'womens-hockey', 'mens-swimming', 'womens-swimming']\n",
    "\n",
    "all_names = []\n",
    "\n",
    "for level in levels:\n",
    "    for sport in sports:\n",
    "\n",
    "        URL = f'https://www.on3.com/nil/rankings/player/{level}/{sport}/'\n",
    "\n",
    "        page = requests.get(URL)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        results = soup.find(class_=\"NilRankingsPageComponent_nilRankingsList__t14Ao\")\n",
    "\n",
    "        try:\n",
    "            names_sport = results.find_all('a', class_=\"MuiTypography-root MuiLink-root MuiLink-underlineNone MuiTypography-h5 MuiTypography-colorPrimary\")\n",
    "            for name in names_sport:\n",
    "                all_names.append(name)\n",
    "        except:\n",
    "            # print(\"Whoops! No athletes appear to have NIL deal for {} {}\".format(level, sport))\n",
    "            continue\n",
    "    #end\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n"
     ]
    }
   ],
   "source": [
    "print(len(all_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there were only 414 athletes using this section of the On3 database... This is such a small number of athletes that we should investiage looking at different sections of the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Selenium & Beatuiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50 50 50 50 50\n",
      "Arch Manning 2023 - present Freshman - 99.53 Isidore Newman New Orleans, LA\n"
     ]
    }
   ],
   "source": [
    "sports = ['football']\n",
    "#years = ['2022','2023','2024','2025','2026']\n",
    "years = ['2023']\n",
    "\n",
    "exp = []\n",
    "athlete_grade = []\n",
    "ages = []\n",
    "ranks = []\n",
    "high_school = []\n",
    "home_town = []\n",
    "colleges = []\n",
    "college_status = []\n",
    "college_distance = []\n",
    "num_offers = []\n",
    "NIL_val = []\n",
    "instagram_followers = []\n",
    "twitter_followers = []\n",
    "tiktok_followers = []\n",
    "\n",
    "for year in years:\n",
    "    for sport in sports:\n",
    "        \n",
    "        # Selenium Driver to click on \"Load More\"\n",
    "        driver = webdriver.Chrome()\n",
    "        URL = f'https://www.on3.com/db/rankings/industry-player/{sport}/{year}/'\n",
    "        driver.get(URL)\n",
    "        \n",
    "        # while True:\n",
    "        #     try:\n",
    "        #         load_more_button = WebDriverWait(driver, 10).until(\n",
    "        #             EC.element_to_be_clickable((By.XPATH, \"//span[@class='MuiButton-label' and contains(text(), 'Load More')]\"))\n",
    "        #         )\n",
    "        #         load_more_button.click()\n",
    "        #         time.sleep(10)\n",
    "        #     except:\n",
    "        #         break\n",
    "        \n",
    "        # Beautiful Soup - for each player\n",
    "        page_source = driver.page_source\n",
    "        page_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        results = page_soup.find(class_=\"PlayerRankings_playerRankings__jvfFg\")\n",
    "\n",
    "        athletes = results.find_all('a', class_=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-h5 MuiTypography-colorPrimary\")\n",
    "        # It is possible that two athletes have the same name: John Smith\n",
    "        # We cannot use a dictionary so a list of tuples is used for now\n",
    "        names = [athlete.text for athlete in athletes]\n",
    "\n",
    "        # Generate a list of links that we can iterate through\n",
    "        links = [athlete['href'] for athlete in athletes]\n",
    "        base_url = \"https://www.on3.com/\"\n",
    "        athlete_links = [urljoin(base_url, link) for link in links]\n",
    "\n",
    "        for athlete_link in athlete_links:\n",
    "            # We can now go into each individual athletes page\n",
    "            # for that sport, for that year, after loading more\n",
    "            try:\n",
    "                driver.get(athlete_link)\n",
    "                time.sleep(3)\n",
    "\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "\n",
    "                CollegeRankingInfo = page_soup.find(class_='CollegeRanking_info__LM3nn')\n",
    "\n",
    "                if CollegeRankingInfo:\n",
    "                    exp_year = CollegeRankingInfo.find_all(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    if len(exp_year) == 2:\n",
    "                        exp.append(exp_year[0].text)\n",
    "                        athlete_grade.append(exp_year[1].text)\n",
    "                    else:\n",
    "                        exp.append(np.nan)\n",
    "                        athlete_grade.append(np.nan)\n",
    "                    age = CollegeRankingInfo.find(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorPrimary')\n",
    "                    ages.append(age.text)\n",
    "                else:\n",
    "                    exp.append(np.nan)\n",
    "                    athlete_grade.append(np.nan)\n",
    "                    ages.append(np.nan)\n",
    "                \n",
    "                RecruitModuleInfo = page_soup.find(class_='RecruitModule_info__Ugxqd')\n",
    "\n",
    "                if RecruitModuleInfo:\n",
    "                    ranking = RecruitModuleInfo.find(class_='RecruitModule_rating__sONqb')\n",
    "                    ranks.append(ranking.text)\n",
    "\n",
    "                    homeInfo = RecruitModuleInfo.find_all(class_='MuiTypography-root RecruitModule_span__KmmzN MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    high_school.append(homeInfo[-2].text)\n",
    "                    home_town.append(homeInfo[-1].text)\n",
    "                else:\n",
    "                    ranks.append(np.nan)\n",
    "                    high_school.append(np.nan)\n",
    "                    home_town.append(np.nan)\n",
    "            except:\n",
    "                print(\"Link {} is no good - removing athlete altogether\".format(athlete_link))\n",
    "                badLink = athlete_link\n",
    "                index = athlete_link.index(badLink)\n",
    "                names.pop(index)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # Move to Recruiting Page\n",
    "            driver.get(urljoin(athlete_link,'recruiting/'))\n",
    "            time.sleep(3)\n",
    "            athlete_source = driver.page_source\n",
    "            page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            # Find url for all teams and navigate to page\n",
    "            all_team_link = page_soup.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineHover PlayerInterestsModule_text__kjqNU MuiTypography-caption MuiTypography-colorPrimary')\n",
    "            driver.get(urljoin(base_url,all_team_link['href']))\n",
    "            time.sleep(3)\n",
    "            athlete_source = driver.page_source\n",
    "            page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            try:\n",
    "                RecruitColleges = page_soup.find_all(class_='PlayerInterestsItem_teamContainer__vjQkf')\n",
    "                if RecruitColleges:\n",
    "                    dummy_colleges=[]\n",
    "                    count = 0\n",
    "                \n",
    "                    for college in RecruitColleges:\n",
    "                        college_name = college.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineNone PlayerInterestsItem_teamName__FeBHv MuiTypography-h5 MuiTypography-colorPrimary')\n",
    "                        college_statuses = college.find(class_='MuiTypography-root PlayerInterestsItem_status__1_rA8 MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                        college_dist = college.find(class_='MuiTypography-root PlayerInterestsItem_distanceText__KJhj3 MuiTypography-caption MuiTypography-colorTextPrimary')\n",
    "                        if count == 0:\n",
    "                            colleges.append(college_name.text)\n",
    "                            college_status.append(college_statuses.text)\n",
    "                            college_distance.append(college_dist.text)\n",
    "                        count +=1\n",
    "                    num_offers.append(count)\n",
    "                else:\n",
    "                    colleges.append(np.nan)\n",
    "                    college_status.append(np.nan)\n",
    "                    college_distance.append(np.nan)\n",
    "                    num_offers.append(np.nan)\n",
    "                    \n",
    "            except:\n",
    "                print(\"error on recruiting page\")\n",
    "            \n",
    "            # Move to NIL Page\n",
    "            driver.get(urljoin(athlete_link,'nil/'))\n",
    "            time.sleep(3)\n",
    "            athlete_source = driver.page_source\n",
    "            page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            try:\n",
    "                RecruitNIL = page_soup.find(class_='NilValuationCircle_nilCircleValue__wzomB')\n",
    "                if RecruitNIL:\n",
    "                    NIL_val.append(RecruitNIL.text)\n",
    "                else:\n",
    "                    NIL_val.append(np.nan)\n",
    "                    \n",
    "            except:\n",
    "                print('error on NIL val')\n",
    "            \n",
    "            try:       \n",
    "                #Need to figure out a way to get each social that is available and determine which it is from the URL possibly\n",
    "                RecruitSocials = page_soup.find(class_=\"NilSocialValuations_socialValuations__MeR7O\")\n",
    "                if RecruitSocials:\n",
    "                    socials = RecruitSocials.find_all(class_=\"NilSocialValuations_platform__3qBMy\")\n",
    "                    instagram = np.nan\n",
    "                    twitter = np.nan\n",
    "                    tiktok =np.nan\n",
    "                    for social in socials:\n",
    "                        social_name = social.find('a')['href']\n",
    "                        if 'instagram' in social_name:\n",
    "                            instagram = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                        elif 'twitter' in social_name:\n",
    "                            twitter = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                        elif 'tiktok' in social_name:\n",
    "                            tiktok = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                    \n",
    "                else:\n",
    "                    instagram = np.nan\n",
    "                    twitter = np.nan\n",
    "                    tiktok =np.nan\n",
    "                \n",
    "                instagram_followers.append(instagram)\n",
    "                twitter_followers.append(twitter)\n",
    "                tiktok_followers.append(tiktok)\n",
    "                \n",
    "            except:\n",
    "                print(\"error on social media\")\n",
    "                \n",
    "        #end\n",
    "        print(len(names), len(exp), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town))\n",
    "        print(names[0], exp[0], athlete_grade[0], ages[0], ranks[0], high_school[0], home_town[0])\n",
    "    #end\n",
    "#end\n",
    "\n",
    "        # This successfully takes us back to the previous page, but it might require us to reload every athlete...\n",
    "        # driver.back()\n",
    "        # url = driver.current_url\n",
    "        # print(url)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['200K', '15.4K', '42K', '136K', '82K', '169K', '16.4K', '25K', '29K', '1.3K', '75K', '13.2K', '97K', '5.4K', '13.7K', '22K', '13.7K', '21K', '23K', '15.5K', '60K', '935', '13.3K', '6.7K', '9.7K', '1.5K', '10.2K', '15.6K', '25K', '22K', '77K', '3.1K', '36K', '35K', '77K', '22K', '16.9K', '24K', '11.8K', '30K', '39K', '7.6K', '6.4K', '14.4K', '14.2K', '15.3K', '37K', '28K', '26K', '5.6K']\n",
      "['65K', '6.6K', '15K', '9.3K', '9.6K', '25K', '7.8K', '31K', '14.2K', '9.1K', '11.2K', '7.5K', '12.6K', '6.5K', '16.3K', '13.1K', '12.9K', '10K', '6.2K', '4.4K', '15.8K', '2.2K', '4.2K', '2.7K', '4.8K', '2.1K', '3.2K', '7.5K', '5.9K', '9.2K', '17.8K', '3.1K', '7.3K', '11.3K', '15.6K', '14.6K', '3.7K', '8.3K', '6.4K', '3K', '19.2K', '3.6K', '2.6K', '11.5K', '10.9K', '5.3K', '6.2K', '11.7K', '8.1K', '3.1K']\n",
      "['8.9K', '1.2K', '6.9K', '78K', '7K', '139K', nan, nan, '2.3K', nan, '83K', '1.4K', '5.2K', '5.2K', nan, nan, '7.2K', '300', nan, '1.8K', '14.5K', nan, '300', nan, '3K', nan, nan, nan, '58K', '1.7K', '136K', '1', '22K', '543', '24K', '12.5K', nan, '1.5K', nan, '7.5K', '6.6K', '1.5K', '591', nan, '1.7K', '3.1K', '53K', nan, '8.1K', '5.4K']\n"
     ]
    }
   ],
   "source": [
    "print(instagram_followers)\n",
    "print(twitter_followers)\n",
    "print(tiktok_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n"
     ]
    }
   ],
   "source": [
    "print(len(names), len(exp), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town), len(colleges), len(college_status), len(college_distance), len(num_offers), len(NIL_val),len(instagram_followers),\n",
    "      len(twitter_followers),len(tiktok_followers))\n",
    "#print(names[253], exp[-2], athlete_grade[-2], ages[-2], ranks[-2], high_school[-2], home_town[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.on3.com/db/bobby-durkin-156822/\n",
      "https://www.on3.com/db/john-blackwell-153825/\n",
      "https://www.on3.com/db/comeh-emuobor-147246/\n",
      "https://www.on3.com/db/malik-olafioye-81952/\n",
      "https://www.on3.com/db/wesley-tubbs-iii-150899/\n",
      "https://www.on3.com/db/jalen-hooks-54341/\n",
      "https://www.on3.com/db/austin-ball-147266/\n",
      "https://www.on3.com/db/chuck-bailey-iii-147243/\n",
      "https://www.on3.com/db/gabe-sisk-44602/\n",
      "https://www.on3.com/db/isaiah-manning-147244/\n"
     ]
    }
   ],
   "source": [
    "for athlete_link in athlete_links[250:260]:\n",
    "    print(athlete_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sports = ['basketball']\n",
    "# #years = ['2022','2023','2024','2025','2026']\n",
    "# years = ['2023']\n",
    "\n",
    "\n",
    "# urls = {}\n",
    "\n",
    "# for year in years:\n",
    "#     for sport in sports:\n",
    "\n",
    "        \n",
    "            \n",
    "        # page = requests.get(URL)\n",
    "        # soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        #results = soup.find(class_=\"PlayerRankings_playerRankings__jvfFg\")\n",
    "        \n",
    "        # page_soup = soup(driver.page_source, 'html.parser')\n",
    "        # print(page_soup)\n",
    "        # results = page_soup.find(class_=\"PlayerRankings_playerRankings__jvfFg\")\n",
    "        # try:\n",
    "        #     names_sport = results.find_all('a', class_=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-h5 MuiTypography-colorPrimary\")\n",
    "        #     for name in names_sport:\n",
    "        #         urls[name.text] = name['href']\n",
    "        # except:\n",
    "        #     print(\"Whoops! No athletes appear to have NIL deal for {} {}\".format(level, sport))\n",
    "        #     continue\n",
    "    #end\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_info = {}\n",
    "\n",
    "# for name, url in urls.items():\n",
    "#     all_info[name] = []\n",
    "#     URL = f'https://www.on3.com{url}/recruiting'\n",
    "#     page = requests.get(URL)\n",
    "#     soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     results = soup.find(class_=\"PlayerRecruiting_playergrid__wYlb_\")\n",
    "#     player_info = results.find_all('span', class_=\"MuiTypography-root MeasurementInfo_text__dCryI MuiTypography-body1 MuiTypography-colorTextPrimary\")\n",
    "#     for info in player_info:\n",
    "#         all_info[name].append(info.text)\n",
    "    \n",
    "#     results = soup.find(class_=\"Rankings_container__U2afk\")\n",
    "#     rankings_info = results.find_all('p', class_=\"MuiTypography-root Rankings_industryRating__9uavm MuiTypography-body1 MuiTypography-colorTextPrimary\")\n",
    "#     for info in rankings_info:\n",
    "#         all_info[name].append(info.text)\n",
    "    \n",
    "    \n",
    "#     results = soup.find(class_=\"PlayerInterestsModule_targets__rxz4U\")\n",
    "#     recruitment_info = results.find_all(['a','h6'], class_=[\"MuiTypography-root MuiLink-root MuiLink-underlineNone PlayerInterestsItem_teamName__FeBHv MuiTypography-h5 MuiTypography-colorPrimary\",\"MuiTypography-root PlayerInterestsItem_predictionLabel__mT201 MuiTypography-subtitle1 MuiTypography-colorTextPrimary\"])\n",
    "#     for info in  recruitment_info:\n",
    "#         all_info[name].append(info.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
