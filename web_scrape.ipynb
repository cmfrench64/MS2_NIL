{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Attempt just using Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = ['college', 'high-school']\n",
    "sports = ['football', 'basketball', 'baseball', 'womens-basketball', 'volleyball', 'gymnastics',\n",
    "         'mens-lacrosse', 'womens-lacrosse', 'mens-soccer', 'womens-soccer', 'softball', 'womens-track',\n",
    "         'mens-golf', 'womens-golf', 'mens-hockey', 'womens-hockey', 'mens-swimming', 'womens-swimming']\n",
    "\n",
    "all_names = []\n",
    "\n",
    "for level in levels:\n",
    "    for sport in sports:\n",
    "\n",
    "        URL = f'https://www.on3.com/nil/rankings/player/{level}/{sport}/'\n",
    "\n",
    "        page = requests.get(URL)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        results = soup.find(class_=\"NilRankingsPageComponent_nilRankingsList__t14Ao\")\n",
    "\n",
    "        try:\n",
    "            names_sport = results.find_all('a', class_=\"MuiTypography-root MuiLink-root MuiLink-underlineNone MuiTypography-h5 MuiTypography-colorPrimary\")\n",
    "            for name in names_sport:\n",
    "                all_names.append(name)\n",
    "        except:\n",
    "            # print(\"Whoops! No athletes appear to have NIL deal for {} {}\".format(level, sport))\n",
    "            continue\n",
    "    #end\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n"
     ]
    }
   ],
   "source": [
    "print(len(all_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there were only 414 athletes using this section of the On3 database... This is such a small number of athletes that we should investiage looking at different sections of the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Selenium & Beatuiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$2.9M</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$455K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$848K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$704K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$609K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$1.1M</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$440K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$759K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$467K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$379K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$866K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$323K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$441K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$281K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$347K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$497K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$300K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$398K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$329K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$319K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$403K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$268K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$371K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$281K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$240K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$218K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$269K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$238K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$252K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$443K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$372K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$199K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$354K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$260K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$317K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$239K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$324K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$241K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$204K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$209K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$237K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$187K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$177K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$198K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$203K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$193K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$217K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$211K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$257K</span>]\n",
      "[<span class=\"NilValuationCircle_nilCircleValue__wzomB\">$198K</span>]\n",
      "50 50 50 50 50 50 50\n",
      "Arch Manning 2023 - present Freshman - 99.53 Isidore Newman New Orleans, LA\n"
     ]
    }
   ],
   "source": [
    "sports = ['football']\n",
    "#years = ['2022','2023','2024','2025','2026']\n",
    "years = ['2023']\n",
    "\n",
    "exp = []\n",
    "athlete_grade = []\n",
    "ages = []\n",
    "ranks = []\n",
    "high_school = []\n",
    "home_town = []\n",
    "colleges = []\n",
    "college_status = []\n",
    "college_distance = []\n",
    "num_offers = []\n",
    "NIL_val = []\n",
    "tot_followers = []\n",
    "\n",
    "for year in years:\n",
    "    for sport in sports:\n",
    "        \n",
    "        # Selenium Driver to click on \"Load More\"\n",
    "        driver = webdriver.Chrome()\n",
    "        URL = f'https://www.on3.com/db/rankings/industry-player/{sport}/{year}/'\n",
    "        driver.get(URL)\n",
    "        \n",
    "        # while True:\n",
    "        #     try:\n",
    "        #         load_more_button = WebDriverWait(driver, 10).until(\n",
    "        #             EC.element_to_be_clickable((By.XPATH, \"//span[@class='MuiButton-label' and contains(text(), 'Load More')]\"))\n",
    "        #         )\n",
    "        #         load_more_button.click()\n",
    "        #         time.sleep(10)\n",
    "        #     except:\n",
    "        #         break\n",
    "        \n",
    "        # Beautiful Soup - for each player\n",
    "        page_source = driver.page_source\n",
    "        page_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        results = page_soup.find(class_=\"PlayerRankings_playerRankings__jvfFg\")\n",
    "\n",
    "        athletes = results.find_all('a', class_=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-h5 MuiTypography-colorPrimary\")\n",
    "        # It is possible that two athletes have the same name: John Smith\n",
    "        # We cannot use a dictionary so a list of tuples is used for now\n",
    "        names = [athlete.text for athlete in athletes]\n",
    "\n",
    "        # Generate a list of links that we can iterate through\n",
    "        links = [athlete['href'] for athlete in athletes]\n",
    "        base_url = \"https://www.on3.com/\"\n",
    "        athlete_links = [urljoin(base_url, link) for link in links]\n",
    "\n",
    "        for athlete_link in athlete_links:\n",
    "            # We can now go into each individual athletes page\n",
    "            # for that sport, for that year, after loading more\n",
    "            try:\n",
    "                driver.get(athlete_link)\n",
    "                time.sleep(3)\n",
    "\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "\n",
    "                CollegeRankingInfo = page_soup.find(class_='CollegeRanking_info__LM3nn')\n",
    "\n",
    "                if CollegeRankingInfo:\n",
    "                    exp_year = CollegeRankingInfo.find_all(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    if len(exp_year) == 2:\n",
    "                        exp.append(exp_year[0].text)\n",
    "                        athlete_grade.append(exp_year[1].text)\n",
    "                    else:\n",
    "                        exp.append(np.nan)\n",
    "                        athlete_grade.append(np.nan)\n",
    "                    age = CollegeRankingInfo.find(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorPrimary')\n",
    "                    ages.append(age.text)\n",
    "                else:\n",
    "                    exp.append(np.nan)\n",
    "                    athlete_grade.append(np.nan)\n",
    "                    ages.append(np.nan)\n",
    "                \n",
    "                RecruitModuleInfo = page_soup.find(class_='RecruitModule_info__Ugxqd')\n",
    "\n",
    "                if RecruitModuleInfo:\n",
    "                    ranking = RecruitModuleInfo.find(class_='RecruitModule_rating__sONqb')\n",
    "                    ranks.append(ranking.text)\n",
    "\n",
    "                    homeInfo = RecruitModuleInfo.find_all(class_='MuiTypography-root RecruitModule_span__KmmzN MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    high_school.append(homeInfo[-2].text)\n",
    "                    home_town.append(homeInfo[-1].text)\n",
    "                else:\n",
    "                    ranks.append(np.nan)\n",
    "                    high_school.append(np.nan)\n",
    "                    home_town.append(np.nan)\n",
    "            except:\n",
    "                print(\"Link {} is no good - removing athlete altogether\".format(athlete_link))\n",
    "                badLink = athlete_link\n",
    "                index = athlete_link.index(badLink)\n",
    "                names.pop(index)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # Move to Recruiting Page\n",
    "            driver.get(urljoin(athlete_link,'recruiting/'))\n",
    "            time.sleep(3)\n",
    "            athlete_source = driver.page_source\n",
    "            page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            # Find url for all teams and navigate to page\n",
    "            all_team_link = page_soup.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineHover PlayerInterestsModule_text__kjqNU MuiTypography-caption MuiTypography-colorPrimary')\n",
    "            driver.get(urljoin(base_url,all_team_link['href']))\n",
    "            time.sleep(3)\n",
    "            athlete_source = driver.page_source\n",
    "            page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            try:\n",
    "                RecruitColleges = page_soup.find_all(class_='PlayerInterestsItem_teamContainer__vjQkf')\n",
    "                dummy_colleges=[]\n",
    "                count = 0\n",
    "                \n",
    "                for college in RecruitColleges:\n",
    "                    college_name = college.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineNone PlayerInterestsItem_teamName__FeBHv MuiTypography-h5 MuiTypography-colorPrimary')\n",
    "                    college_statuses = college.find(class_='MuiTypography-root PlayerInterestsItem_status__1_rA8 MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    college_dist = college.find(class_='MuiTypography-root PlayerInterestsItem_distanceText__KJhj3 MuiTypography-caption MuiTypography-colorTextPrimary')\n",
    "                    if count == 0:\n",
    "                        colleges.append(college_name.text)\n",
    "                        college_status.append(college_statuses.text)\n",
    "                        college_distance.append(college_dist.text)\n",
    "                    count +=1\n",
    "                num_offers.append(count)\n",
    "            except:\n",
    "                print(\"error on recruiting page\")\n",
    "            \n",
    "            # Move to NIL Page\n",
    "            driver.get(urljoin(athlete_link,'nil/'))\n",
    "            time.sleep(3)\n",
    "            athlete_source = driver.page_source\n",
    "            page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            try:\n",
    "                RecruitNIL = page_soup.find_all(class_='NilValuationCircle_nilCircleValue__wzomB')\n",
    "                if RecruitNIL:\n",
    "                    NIL_val.append(RecruitNIL.text)\n",
    "                else:\n",
    "                    NIL_val.append(np.nan)\n",
    "                RecruitFollowers = page_soup.find_all(class_='NilSocialValuations_valuationNumber__nB_Xc MuiTypography-h1 MuiTypography-colorTextPrimary')\n",
    "                if RecruitFollowers:\n",
    "                    tot_followers.append(RecruitFollowers.text)\n",
    "                else:\n",
    "                    tot_followers.append(np.nan)\n",
    "                #Need to figure out a way to get each social that is available and determine which it is from the URL possibly\n",
    "                #RecruitSocials = page_soup.find_all(class_=\"NilSocialValuations_platform__3qBMy\")\n",
    "                \n",
    "            except:\n",
    "                print(\"error on NIL page\")\n",
    "                \n",
    "        #end\n",
    "        print(len(names), len(exp), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town))\n",
    "        print(names[0], exp[0], athlete_grade[0], ages[0], ranks[0], high_school[0], home_town[0])\n",
    "    #end\n",
    "#end\n",
    "\n",
    "        # This successfully takes us back to the previous page, but it might require us to reload every athlete...\n",
    "        # driver.back()\n",
    "        # url = driver.current_url\n",
    "        # print(url)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50 50 50 50 50 50 50 50 50\n"
     ]
    }
   ],
   "source": [
    "print(len(names), len(exp), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town), len(colleges), len(college_status), len(college_distance), len(num_offers))\n",
    "#print(names[253], exp[-2], athlete_grade[-2], ages[-2], ranks[-2], high_school[-2], home_town[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.on3.com/db/bobby-durkin-156822/\n",
      "https://www.on3.com/db/john-blackwell-153825/\n",
      "https://www.on3.com/db/comeh-emuobor-147246/\n",
      "https://www.on3.com/db/malik-olafioye-81952/\n",
      "https://www.on3.com/db/wesley-tubbs-iii-150899/\n",
      "https://www.on3.com/db/jalen-hooks-54341/\n",
      "https://www.on3.com/db/austin-ball-147266/\n",
      "https://www.on3.com/db/chuck-bailey-iii-147243/\n",
      "https://www.on3.com/db/gabe-sisk-44602/\n",
      "https://www.on3.com/db/isaiah-manning-147244/\n"
     ]
    }
   ],
   "source": [
    "for athlete_link in athlete_links[250:260]:\n",
    "    print(athlete_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sports = ['basketball']\n",
    "# #years = ['2022','2023','2024','2025','2026']\n",
    "# years = ['2023']\n",
    "\n",
    "\n",
    "# urls = {}\n",
    "\n",
    "# for year in years:\n",
    "#     for sport in sports:\n",
    "\n",
    "        \n",
    "            \n",
    "        # page = requests.get(URL)\n",
    "        # soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        #results = soup.find(class_=\"PlayerRankings_playerRankings__jvfFg\")\n",
    "        \n",
    "        # page_soup = soup(driver.page_source, 'html.parser')\n",
    "        # print(page_soup)\n",
    "        # results = page_soup.find(class_=\"PlayerRankings_playerRankings__jvfFg\")\n",
    "        # try:\n",
    "        #     names_sport = results.find_all('a', class_=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-h5 MuiTypography-colorPrimary\")\n",
    "        #     for name in names_sport:\n",
    "        #         urls[name.text] = name['href']\n",
    "        # except:\n",
    "        #     print(\"Whoops! No athletes appear to have NIL deal for {} {}\".format(level, sport))\n",
    "        #     continue\n",
    "    #end\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_info = {}\n",
    "\n",
    "# for name, url in urls.items():\n",
    "#     all_info[name] = []\n",
    "#     URL = f'https://www.on3.com{url}/recruiting'\n",
    "#     page = requests.get(URL)\n",
    "#     soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     results = soup.find(class_=\"PlayerRecruiting_playergrid__wYlb_\")\n",
    "#     player_info = results.find_all('span', class_=\"MuiTypography-root MeasurementInfo_text__dCryI MuiTypography-body1 MuiTypography-colorTextPrimary\")\n",
    "#     for info in player_info:\n",
    "#         all_info[name].append(info.text)\n",
    "    \n",
    "#     results = soup.find(class_=\"Rankings_container__U2afk\")\n",
    "#     rankings_info = results.find_all('p', class_=\"MuiTypography-root Rankings_industryRating__9uavm MuiTypography-body1 MuiTypography-colorTextPrimary\")\n",
    "#     for info in rankings_info:\n",
    "#         all_info[name].append(info.text)\n",
    "    \n",
    "    \n",
    "#     results = soup.find(class_=\"PlayerInterestsModule_targets__rxz4U\")\n",
    "#     recruitment_info = results.find_all(['a','h6'], class_=[\"MuiTypography-root MuiLink-root MuiLink-underlineNone PlayerInterestsItem_teamName__FeBHv MuiTypography-h5 MuiTypography-colorPrimary\",\"MuiTypography-root PlayerInterestsItem_predictionLabel__mT201 MuiTypography-subtitle1 MuiTypography-colorTextPrimary\"])\n",
    "#     for info in  recruitment_info:\n",
    "#         all_info[name].append(info.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
