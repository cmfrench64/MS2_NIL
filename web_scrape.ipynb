{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Selenium & Beatuiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully recovered 50 athletes for basketball 2023!\n",
      "Starting Scrape\n",
      "=== Scarped 0.1 % of the Players ===\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=117.0.5938.88)\nStacktrace:\n0   chromedriver                        0x00000001012e2d58 chromedriver + 4336984\n1   chromedriver                        0x00000001012dadd4 chromedriver + 4304340\n2   chromedriver                        0x0000000100f07a5c chromedriver + 293468\n3   chromedriver                        0x0000000100ee06c0 chromedriver + 132800\n4   chromedriver                        0x0000000100f7404c chromedriver + 737356\n5   chromedriver                        0x0000000100f87198 chromedriver + 815512\n6   chromedriver                        0x0000000100f40a5c chromedriver + 526940\n7   chromedriver                        0x0000000100f41908 chromedriver + 530696\n8   chromedriver                        0x00000001012a8da4 chromedriver + 4099492\n9   chromedriver                        0x00000001012ad260 chromedriver + 4117088\n10  chromedriver                        0x00000001012b34ec chromedriver + 4142316\n11  chromedriver                        0x00000001012add60 chromedriver + 4119904\n12  chromedriver                        0x0000000101285a34 chromedriver + 3955252\n13  chromedriver                        0x00000001012caa08 chromedriver + 4237832\n14  chromedriver                        0x00000001012cab84 chromedriver + 4238212\n15  chromedriver                        0x00000001012daa4c chromedriver + 4303436\n16  libsystem_pthread.dylib             0x00000001951cbfa8 _pthread_start + 148\n17  libsystem_pthread.dylib             0x00000001951c6da0 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 191\u001b[0m\n\u001b[1;32m    189\u001b[0m driver\u001b[39m.\u001b[39mget(urljoin(athlete_link,\u001b[39m'\u001b[39m\u001b[39mnil/\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    190\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m3\u001b[39m)\n\u001b[0;32m--> 191\u001b[0m athlete_source \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39;49mpage_source\n\u001b[1;32m    192\u001b[0m page_soup \u001b[39m=\u001b[39m BeautifulSoup(athlete_source, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    194\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/mvizzini951/MS2_NIL/venv/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:445\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpage_source\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    438\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \n\u001b[1;32m    440\u001b[0m \u001b[39m    :Usage:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39m            driver.page_source\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET_PAGE_SOURCE)[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/mvizzini951/MS2_NIL/venv/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m--> 344\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[1;32m    345\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/mvizzini951/MS2_NIL/venv/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=117.0.5938.88)\nStacktrace:\n0   chromedriver                        0x00000001012e2d58 chromedriver + 4336984\n1   chromedriver                        0x00000001012dadd4 chromedriver + 4304340\n2   chromedriver                        0x0000000100f07a5c chromedriver + 293468\n3   chromedriver                        0x0000000100ee06c0 chromedriver + 132800\n4   chromedriver                        0x0000000100f7404c chromedriver + 737356\n5   chromedriver                        0x0000000100f87198 chromedriver + 815512\n6   chromedriver                        0x0000000100f40a5c chromedriver + 526940\n7   chromedriver                        0x0000000100f41908 chromedriver + 530696\n8   chromedriver                        0x00000001012a8da4 chromedriver + 4099492\n9   chromedriver                        0x00000001012ad260 chromedriver + 4117088\n10  chromedriver                        0x00000001012b34ec chromedriver + 4142316\n11  chromedriver                        0x00000001012add60 chromedriver + 4119904\n12  chromedriver                        0x0000000101285a34 chromedriver + 3955252\n13  chromedriver                        0x00000001012caa08 chromedriver + 4237832\n14  chromedriver                        0x00000001012cab84 chromedriver + 4238212\n15  chromedriver                        0x00000001012daa4c chromedriver + 4303436\n16  libsystem_pthread.dylib             0x00000001951cbfa8 _pthread_start + 148\n17  libsystem_pthread.dylib             0x00000001951c6da0 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "sports = ['basketball']\n",
    "#years = ['2022','2023','2024','2025','2026']\n",
    "years = ['2023']\n",
    "\n",
    "exp = []\n",
    "athlete_grade = []\n",
    "pos_height_weight = []\n",
    "ages = []\n",
    "ranks = []\n",
    "high_school = []\n",
    "home_town = []\n",
    "colleges = []\n",
    "college_status = []\n",
    "college_distance = []\n",
    "num_offers = []\n",
    "NIL_val = []\n",
    "instagram_followers = []\n",
    "twitter_followers = []\n",
    "tiktok_followers = []\n",
    "\n",
    "for year in years:\n",
    "    for sport in sports:\n",
    "        \n",
    "        # Selenium Driver to click on \"Load More\"\n",
    "        driver = webdriver.Chrome()\n",
    "        URL = f'https://www.on3.com/db/rankings/industry-player/{sport}/{year}/'\n",
    "        driver.get(URL)\n",
    "\n",
    "        dummyCount = 0\n",
    "        \n",
    "        ###########################################################################################################################\n",
    "        # Click the \"Load More\" button such that all athelete links are visible\n",
    "        # dummyCount was implemented for sports/years in which the number of athletes is very big (ex: football 2023 has 3000+ athletes) AND\n",
    "        # the 'Load More' button does not disappear when pressed to completiion... so a simple counter was implemented\n",
    "        # By making this condition: 'dummyCount < 19' we max out at 1000 athletes\n",
    "        # while (dummyCount < 19):\n",
    "        #     try:\n",
    "        #         load_more_button = WebDriverWait(driver, 10).until(\n",
    "        #             EC.element_to_be_clickable((By.XPATH, \"//span[@class='MuiButton-label' and contains(text(), 'Load More')]\"))\n",
    "        #         )\n",
    "        #         load_more_button.click()\n",
    "        #         time.sleep(10)\n",
    "        #         dummyCount += 1\n",
    "        #     except:\n",
    "        #         break\n",
    "\n",
    "        ###########################################################################################################################\n",
    "        \n",
    "        ###########################################################################################################################\n",
    "        # Get all athlete names & associated links\n",
    "        # New Information: names, links\n",
    "        page_source = driver.page_source\n",
    "        page_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        results = page_soup.find(class_=\"PlayerRankings_playerRankings__jvfFg\")\n",
    "\n",
    "        athletes = results.find_all('a', class_=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-h5 MuiTypography-colorPrimary\")\n",
    "        names = [athlete.text for athlete in athletes]\n",
    "\n",
    "        # Generate a list of links that we can iterate through\n",
    "        links = [athlete['href'] for athlete in athletes]\n",
    "        base_url = \"https://www.on3.com/\"\n",
    "        athlete_links = [urljoin(base_url, link) for link in links]\n",
    "\n",
    "        tot = len(athlete_links)\n",
    "\n",
    "        print('Successfully recovered {} athletes for {} {}!'.format(tot, sport, year))\n",
    "        print('=== Starting Scrape ===')\n",
    "\n",
    "        ###########################################################################################################################\n",
    "\n",
    "        for i, athlete_link in enumerate(athlete_links):\n",
    "\n",
    "            ###########################################################################################################################\n",
    "            # Print something out to the user, like a progress bar\n",
    "            if (i % 5 == 0) and (i != 0):\n",
    "                print(\"=== Scarped {:.2} % of the Players ===\".format((i/tot)*100))\n",
    "            ###########################################################################################################################\n",
    "\n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"Player\" page for each of the athletes\n",
    "            # New Information: exp, athlete_grade, ages, ranks, high_school, home_town, pos_height_weight\n",
    "            try:\n",
    "                driver.get(athlete_link)\n",
    "                time.sleep(3)\n",
    "\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "\n",
    "                CollegeRankingInfo = page_soup.find(class_='CollegeRanking_info__LM3nn')\n",
    "\n",
    "                if CollegeRankingInfo:\n",
    "                    exp_year = CollegeRankingInfo.find_all(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    if len(exp_year) == 2:\n",
    "                        exp.append(exp_year[0].text)\n",
    "                        athlete_grade.append(exp_year[1].text)\n",
    "                    else:\n",
    "                        exp.append(np.nan)\n",
    "                        athlete_grade.append(np.nan)\n",
    "                    age = CollegeRankingInfo.find(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorPrimary')\n",
    "                    ages.append(age.text)\n",
    "                else:\n",
    "                    exp.append(np.nan)\n",
    "                    athlete_grade.append(np.nan)\n",
    "                    ages.append(np.nan)\n",
    "                \n",
    "                RecruitModuleInfo = page_soup.find(class_='RecruitModule_info__Ugxqd')\n",
    "\n",
    "                if RecruitModuleInfo:\n",
    "                    ranking = RecruitModuleInfo.find(class_='RecruitModule_rating__sONqb')\n",
    "                    ranks.append(ranking.text)\n",
    "\n",
    "                    homeInfo = RecruitModuleInfo.find_all(class_='MuiTypography-root RecruitModule_span__KmmzN MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    high_school.append(homeInfo[-2].text)\n",
    "                    home_town.append(homeInfo[-1].text)\n",
    "                else:\n",
    "                    ranks.append(np.nan)\n",
    "                    high_school.append(np.nan)\n",
    "                    home_town.append(np.nan)\n",
    "\n",
    "                Attributes = page_soup.find(class_='MeasurementInfo_info__IHmGD')\n",
    "\n",
    "                if Attributes:\n",
    "                    dummy = Attributes.find_all(class_='MuiTypography-root MeasurementInfo_text__dCryI MuiTypography-body1 MuiTypography-colorTextPrimary')\n",
    "                    if len(dummy) == 2:\n",
    "                        pos_height_weight.append(dummy[1].text)\n",
    "                    else:\n",
    "                        pos_height_weight.append(np.nan)\n",
    "            except:\n",
    "                print(\"Error on the 'Player' page for athlete: {}\".format(names[i]))\n",
    "                print(\"Removing all previous information about this player! (i.e name)\")\n",
    "                names.pop(i)\n",
    "                continue\n",
    "            ###########################################################################################################################\n",
    "            \n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"Recruiting\" page for each of the athletes\n",
    "            # New Information: colleges (the college this athlete is targeting), college_status, college_distance, num_offers\n",
    "            driver.get(urljoin(athlete_link,'recruiting/'))\n",
    "            time.sleep(3)\n",
    "            athlete_source = driver.page_source\n",
    "            page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            # Find url for all teams and navigate to page\n",
    "            all_team_link = page_soup.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineHover PlayerInterestsModule_text__kjqNU MuiTypography-caption MuiTypography-colorPrimary')\n",
    "            driver.get(urljoin(base_url,all_team_link['href']))\n",
    "            time.sleep(3)\n",
    "            athlete_source = driver.page_source\n",
    "            page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            try:\n",
    "                RecruitColleges = page_soup.find_all(class_='PlayerInterestsItem_teamContainer__vjQkf')\n",
    "                if RecruitColleges:\n",
    "                    dummy_colleges=[]\n",
    "                    count = 0\n",
    "                \n",
    "                    for college in RecruitColleges:\n",
    "                        college_name = college.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineNone PlayerInterestsItem_teamName__FeBHv MuiTypography-h5 MuiTypography-colorPrimary')\n",
    "                        college_statuses = college.find(class_='MuiTypography-root PlayerInterestsItem_status__1_rA8 MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                        college_dist = college.find(class_='MuiTypography-root PlayerInterestsItem_distanceText__KJhj3 MuiTypography-caption MuiTypography-colorTextPrimary')\n",
    "                        if count == 0:\n",
    "                            colleges.append(college_name.text)\n",
    "                            college_status.append(college_statuses.text)\n",
    "                            college_distance.append(college_dist.text)\n",
    "                        count +=1\n",
    "                    num_offers.append(count)\n",
    "                else:\n",
    "                    colleges.append(np.nan)\n",
    "                    college_status.append(np.nan)\n",
    "                    college_distance.append(np.nan)\n",
    "                    num_offers.append(np.nan)\n",
    "                    \n",
    "            except:\n",
    "                print(\"Error on the 'Recruiting' page for athlete: {}\".format(names[i]))\n",
    "                print(\"Removing all previous information about this player! (i.e name, exp, athlete_grade, ages, ranks, high_school, home_town, pos_height_weight)\")\n",
    "                names.pop(i)\n",
    "                exp.pop(i)\n",
    "                athlete_grade.pop(i)\n",
    "                ages.pop(i)\n",
    "                ranks.pop(i)\n",
    "                high_school.pop(i)\n",
    "                home_town.pop(i)\n",
    "                pos_height_weight.pop(i)\n",
    "                continue\n",
    "            ###########################################################################################################################\n",
    "            \n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"NIL\" page for each of the athletes\n",
    "            # New Information: NIL_val, instagram_followers, twitter_followers, tiktok_followers\n",
    "            driver.get(urljoin(athlete_link,'nil/'))\n",
    "            time.sleep(3)\n",
    "            athlete_source = driver.page_source\n",
    "            page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "            \n",
    "            try:\n",
    "                RecruitNIL = page_soup.find(class_='NilValuationCircle_nilCircleValue__wzomB')\n",
    "\n",
    "                if RecruitNIL:\n",
    "                    NIL_val.append(RecruitNIL.text)\n",
    "                else:\n",
    "                    NIL_val.append(np.nan)\n",
    "                \n",
    "                RecruitSocials = page_soup.find(class_=\"NilSocialValuations_socialValuations__MeR7O\")\n",
    "\n",
    "                instagram = np.nan\n",
    "                twitter = np.nan\n",
    "                tiktok = np.nan\n",
    "\n",
    "                if RecruitSocials:\n",
    "                    socials = RecruitSocials.find_all(class_=\"NilSocialValuations_platform__3qBMy\")\n",
    "                    for social in socials:\n",
    "                        social_name = social.find('a')['href']\n",
    "                        if 'instagram' in social_name:\n",
    "                            instagram = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                        elif 'twitter' in social_name:\n",
    "                            twitter = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                        elif 'tiktok' in social_name:\n",
    "                            tiktok = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                \n",
    "                instagram_followers.append(instagram)\n",
    "                twitter_followers.append(twitter)\n",
    "                tiktok_followers.append(tiktok)\n",
    "                \n",
    "            except:\n",
    "                print(\"Error on the 'NIL' page for athlete: {}\".format(names[i]))\n",
    "                print(\"Removing all previous information about this player! (i.e name, exp, athlete_grade, ages, ranks, high_school, home_town, pos_height_weight, colleges, college_status, college_distance, num_offers)\")\n",
    "                names.pop(i)\n",
    "                exp.pop(i)\n",
    "                athlete_grade.pop(i)\n",
    "                ages.pop(i)\n",
    "                ranks.pop(i)\n",
    "                high_school.pop(i)\n",
    "                home_town.pop(i)\n",
    "                pos_height_weight.pop(i)\n",
    "                colleges.pop(i)\n",
    "                college_status.pop(i)\n",
    "                college_distance.pop(i)\n",
    "                num_offers.pop(i)\n",
    "                continue\n",
    "            ###########################################################################################################################\n",
    "        print('Successfully completed {} athletes for {} {}!'.format(len(names), sport, year))\n",
    "        print('Number of removed athletes: {}'.format(len(athlete_links) - len(names)))\n",
    "        #end\n",
    "    #end\n",
    "#end\n",
    "\n",
    "# Close the instance of the webpage\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(instagram_followers)\n",
    "# print(twitter_followers)\n",
    "# print(tiktok_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      "Isaiah Collier 2023 - present PG / 6-3 / 190 Freshman 18 99.39 Wheeler\n",
      "Marietta, GA USC Enrolled 1921 mi. 17\n",
      "$594K 85K 2.8K nan\n"
     ]
    }
   ],
   "source": [
    "print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "      len(colleges), len(college_status), len(college_distance), len(num_offers), len(NIL_val),\n",
    "      len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "print(names[0], exp[0], pos_height_weight[0], athlete_grade[0], ages[0], ranks[0], high_school[0])\n",
    "print(home_town[0], colleges[0], college_status[0], college_distance[0], num_offers[0])\n",
    "print(NIL_val[0], instagram_followers[0], twitter_followers[0], tiktok_followers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
