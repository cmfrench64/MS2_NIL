{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Selenium & Beatuiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully recovered 326 athletes for basketball 2023!\n",
      "========== Starting Scrape ===========\n",
      "=== Scraped 3.07 % of the Players ===\n",
      "326 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      "======================================\n",
      "=== Scraped 6.13 % of the Players ===\n",
      "326 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21\n",
      "======================================\n",
      "=== Scraped 9.20 % of the Players ===\n",
      "326 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31\n",
      "======================================\n",
      "=== Scraped 12.27 % of the Players ===\n",
      "326 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41\n",
      "======================================\n",
      "=== Scraped 15.34 % of the Players ===\n",
      "326 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51\n",
      "======================================\n",
      "=== Scraped 18.40 % of the Players ===\n",
      "326 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61\n",
      "======================================\n",
      "=== Scraped 21.47 % of the Players ===\n",
      "326 71 71 71 71 71 71 71 71 71 71 71 71 71 71 71\n",
      "======================================\n",
      "=== Scraped 24.54 % of the Players ===\n",
      "326 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81\n",
      "======================================\n",
      "=== Scraped 27.61 % of the Players ===\n",
      "326 91 91 91 91 91 91 91 91 91 91 91 91 91 91 91\n",
      "======================================\n",
      "=== Scraped 30.67 % of the Players ===\n",
      "326 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101\n",
      "======================================\n",
      "=== Scraped 33.74 % of the Players ===\n",
      "326 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111\n",
      "======================================\n",
      "=== Scraped 36.81 % of the Players ===\n",
      "326 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      "======================================\n",
      "=== Scraped 39.88 % of the Players ===\n",
      "326 131 131 131 131 131 131 131 131 131 131 131 131 131 131 131\n",
      "======================================\n",
      "=== Scraped 42.94 % of the Players ===\n",
      "326 141 141 141 141 141 141 141 141 141 141 141 141 141 141 141\n",
      "======================================\n",
      "=== Scraped 46.01 % of the Players ===\n",
      "326 151 151 151 151 151 151 151 151 151 151 151 151 151 151 151\n",
      "======================================\n",
      "Error on the 'Recruiting' page for athlete: Aaron McBride\n",
      "Removing all previous information about this player! (i.e name, exp, athlete_grade, ages, ranks, high_school, home_town, pos_height_weight)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m all_team_link \u001b[39m=\u001b[39m page_soup\u001b[39m.\u001b[39mfind(class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMuiTypography-root MuiLink-root MuiLink-underlineHover PlayerInterestsModule_text__kjqNU MuiTypography-caption MuiTypography-colorPrimary\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m driver\u001b[39m.\u001b[39mget(urljoin(base_url,all_team_link[\u001b[39m'\u001b[39;49m\u001b[39mhref\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m     high_school\u001b[39m.\u001b[39mpop(i)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m     home_town\u001b[39m.\u001b[39mpop(i)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m     pos_height_weight\u001b[39m.\u001b[39mpop(i)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m \u001b[39m###########################################################################################################################\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m \u001b[39m###########################################################################################################################\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m \u001b[39m# Information from the \"NIL\" page for each of the athletes\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mvizzini/Documents/mvizzini951/MS2_NIL/web_scrape.ipynb#W2sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m \u001b[39m# New Information: NIL_val, instagram_followers, twitter_followers, tiktok_followers\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop index out of range"
     ]
    }
   ],
   "source": [
    "sports = ['basketball']\n",
    "#years = ['2022','2023','2024','2025','2026']\n",
    "years = ['2023']\n",
    "\n",
    "exp = []\n",
    "athlete_grade = []\n",
    "pos_height_weight = []\n",
    "ages = []\n",
    "ranks = []\n",
    "high_school = []\n",
    "home_town = []\n",
    "colleges = []\n",
    "college_status = []\n",
    "college_distance = []\n",
    "num_offers = []\n",
    "NIL_val = []\n",
    "instagram_followers = []\n",
    "twitter_followers = []\n",
    "tiktok_followers = []\n",
    "\n",
    "for year in years:\n",
    "    for sport in sports:\n",
    "        \n",
    "        # Selenium Driver to click on \"Load More\"\n",
    "        driver = webdriver.Chrome()\n",
    "        URL = f'https://www.on3.com/db/rankings/industry-player/{sport}/{year}/'\n",
    "        driver.get(URL)\n",
    "\n",
    "        dummyCount = 0\n",
    "        \n",
    "        ###########################################################################################################################\n",
    "        # Click the \"Load More\" button such that all athelete links are visible\n",
    "        # dummyCount was implemented for sports/years in which the number of athletes is very big (ex: football 2023 has 3000+ athletes) AND\n",
    "        # the 'Load More' button does not disappear when pressed to completiion... so a simple counter was implemented\n",
    "        # By making this condition: 'dummyCount < 19' we max out at 1000 athletes\n",
    "        while (dummyCount < 19):\n",
    "            try:\n",
    "                load_more_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, \"//span[@class='MuiButton-label' and contains(text(), 'Load More')]\"))\n",
    "                )\n",
    "                load_more_button.click()\n",
    "                time.sleep(10)\n",
    "                dummyCount += 1\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        ###########################################################################################################################\n",
    "        \n",
    "        ###########################################################################################################################\n",
    "        # Get all athlete names & associated links\n",
    "        # New Information: names, links\n",
    "        page_source = driver.page_source\n",
    "        page_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        results = page_soup.find(class_=\"PlayerRankings_playerRankings__jvfFg\")\n",
    "\n",
    "        athletes = results.find_all('a', class_=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-h5 MuiTypography-colorPrimary\")\n",
    "        names = [athlete.text for athlete in athletes]\n",
    "\n",
    "        # Generate a list of links that we can iterate through\n",
    "        links = [athlete['href'] for athlete in athletes]\n",
    "        base_url = \"https://www.on3.com/\"\n",
    "        athlete_links = [urljoin(base_url, link) for link in links]\n",
    "\n",
    "        tot = len(athlete_links)\n",
    "\n",
    "        print('Successfully recovered {} athletes for {} {}!'.format(tot, sport, year))\n",
    "        print('========== Starting Scrape ===========')\n",
    "\n",
    "        ###########################################################################################################################\n",
    "\n",
    "        for i, athlete_link in enumerate(athlete_links):\n",
    "\n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"Player\" page for each of the athletes\n",
    "            # New Information: exp, athlete_grade, ages, high_school, home_town, pos_height_weight\n",
    "            try:\n",
    "                driver.get(athlete_link)\n",
    "                time.sleep(3)\n",
    "\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "\n",
    "                CollegeRankingInfo = page_soup.find(class_='CollegeRanking_info__LM3nn')\n",
    "\n",
    "                if CollegeRankingInfo:\n",
    "                    exp_year = CollegeRankingInfo.find_all(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    if len(exp_year) == 2:\n",
    "                        exp.append(exp_year[0].text)\n",
    "                        athlete_grade.append(exp_year[1].text)\n",
    "                    else:\n",
    "                        exp.append(np.nan)\n",
    "                        athlete_grade.append(np.nan)\n",
    "                    age = CollegeRankingInfo.find(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorPrimary')\n",
    "                    ages.append(age.text)\n",
    "                else:\n",
    "                    exp.append(np.nan)\n",
    "                    athlete_grade.append(np.nan)\n",
    "                    ages.append(np.nan)\n",
    "                \n",
    "                RecruitModuleInfo = page_soup.find(class_='RecruitModule_info__Ugxqd')\n",
    "\n",
    "                if RecruitModuleInfo:\n",
    "                    # Moving over to \"Recruiting\" page as this is more robust for more athletes\n",
    "                    # ranking = RecruitModuleInfo.find(class_='RecruitModule_rating__sONqb')\n",
    "                    # ranks.append(ranking.text)\n",
    "\n",
    "                    homeInfo = RecruitModuleInfo.find_all(class_='MuiTypography-root RecruitModule_span__KmmzN MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    high_school.append(homeInfo[-2].text)\n",
    "                    home_town.append(homeInfo[-1].text)\n",
    "                else:\n",
    "                    # ranks.append(np.nan)\n",
    "                    high_school.append(np.nan)\n",
    "                    home_town.append(np.nan)\n",
    "\n",
    "                Attributes = page_soup.find(class_='MeasurementInfo_info__IHmGD')\n",
    "\n",
    "                if Attributes:\n",
    "                    dummy = Attributes.find_all(class_='MuiTypography-root MeasurementInfo_text__dCryI MuiTypography-body1 MuiTypography-colorTextPrimary')\n",
    "                    if len(dummy) >= 2:\n",
    "                        pos_height_weight.append(dummy[1].text)\n",
    "                    else:\n",
    "                        pos_height_weight.append(np.nan)\n",
    "            \n",
    "            except:\n",
    "                print(\"Error on the 'Player' page for athlete: {}\".format(names[i]))\n",
    "                print(\"Removing all previous information about this player! (i.e name)\")\n",
    "                names.pop(i)\n",
    "                continue\n",
    "            ###########################################################################################################################\n",
    "            \n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"Recruiting\" page for each of the athletes\n",
    "            # New Information: ranks, colleges (the college this athlete is targeting), college_status, college_distance, num_offers\n",
    "            try:\n",
    "                driver.get(urljoin(athlete_link,'recruiting/'))\n",
    "                time.sleep(3)\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "\n",
    "                Ranks = page_soup.find(class_=\"Rankings_industryRankWrapper__2qwnq\")\n",
    "\n",
    "                if Ranks:\n",
    "                    ranking = Ranks.find(class_=\"MuiTypography-root Rankings_industryRating__9uavm MuiTypography-body1 MuiTypography-colorTextPrimary\")\n",
    "                    ranks.append(ranking.text)\n",
    "                else:\n",
    "                    ranks.append(np.nan)\n",
    "            \n",
    "                # Find url for all teams and navigate to page\n",
    "                all_team_link = page_soup.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineHover PlayerInterestsModule_text__kjqNU MuiTypography-caption MuiTypography-colorPrimary')\n",
    "                driver.get(urljoin(base_url,all_team_link['href']))\n",
    "                time.sleep(3)\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "\n",
    "                RecruitColleges = page_soup.find_all(class_='PlayerInterestsItem_teamContainer__vjQkf')\n",
    "\n",
    "                if RecruitColleges:\n",
    "                    dummy_colleges=[]\n",
    "                    count = 0\n",
    "                \n",
    "                    for college in RecruitColleges:\n",
    "                        college_name = college.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineNone PlayerInterestsItem_teamName__FeBHv MuiTypography-h5 MuiTypography-colorPrimary')\n",
    "\n",
    "                        if year == '2024' or year == '2025' or year == '2026':\n",
    "                            college_statuses = college.find(class_='MuiTypography-root PlayerInterestsItem_status__1_rA8 PlayerInterestsItem_offered__OxPV0 MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                        else:\n",
    "                            college_statuses = college.find(class_='MuiTypography-root PlayerInterestsItem_status__1_rA8 MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                        \n",
    "                        college_dist = college.find(class_='MuiTypography-root PlayerInterestsItem_distanceText__KJhj3 MuiTypography-caption MuiTypography-colorTextPrimary')\n",
    "                        if count == 0:\n",
    "                            colleges.append(college_name.text)\n",
    "                            college_status.append(college_statuses.text)\n",
    "                            college_distance.append(college_dist.text)\n",
    "                        count +=1\n",
    "                    num_offers.append(count)\n",
    "                else:\n",
    "                    colleges.append(np.nan)\n",
    "                    college_status.append(np.nan)\n",
    "                    college_distance.append(np.nan)\n",
    "                    num_offers.append(np.nan)\n",
    "                    \n",
    "            except:\n",
    "                print(\"Error on the 'Recruiting' page for athlete: {}\".format(names[i]))\n",
    "                print(\"Removing all previous information about this player! (i.e name, exp, athlete_grade, ages, ranks, high_school, home_town, pos_height_weight)\")\n",
    "                names.pop(i)\n",
    "                exp.pop()\n",
    "                athlete_grade.pop()\n",
    "                ages.pop()\n",
    "                high_school.pop()\n",
    "                home_town.pop()\n",
    "                pos_height_weight.pop()\n",
    "                continue\n",
    "            ###########################################################################################################################\n",
    "            \n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"NIL\" page for each of the athletes\n",
    "            # New Information: NIL_val, instagram_followers, twitter_followers, tiktok_followers\n",
    "            try:\n",
    "                driver.get(urljoin(athlete_link,'nil/'))\n",
    "                time.sleep(3)\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "                \n",
    "                RecruitNIL = page_soup.find(class_='NilValuationCircle_nilCircleValue__wzomB')\n",
    "\n",
    "                if RecruitNIL:\n",
    "                    NIL_val.append(RecruitNIL.text)\n",
    "                else:\n",
    "                    NIL_val.append(np.nan)\n",
    "                \n",
    "                RecruitSocials = page_soup.find(class_=\"NilSocialValuations_socialValuations__MeR7O\")\n",
    "\n",
    "                instagram = np.nan\n",
    "                twitter = np.nan\n",
    "                tiktok = np.nan\n",
    "\n",
    "                if RecruitSocials:\n",
    "                    socials = RecruitSocials.find_all(class_=\"NilSocialValuations_platform__3qBMy\")\n",
    "                    for social in socials:\n",
    "                        social_name = social.find('a')['href']\n",
    "                        if 'instagram' in social_name:\n",
    "                            instagram = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                        elif 'twitter' in social_name:\n",
    "                            twitter = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                        elif 'tiktok' in social_name:\n",
    "                            tiktok = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                \n",
    "                instagram_followers.append(instagram)\n",
    "                twitter_followers.append(twitter)\n",
    "                tiktok_followers.append(tiktok)\n",
    "                \n",
    "            except:\n",
    "                print(\"Error on the 'NIL' page for athlete: {}\".format(names[i]))\n",
    "                print(\"Removing all previous information about this player! (i.e name, exp, athlete_grade, ages, ranks, high_school, home_town, pos_height_weight, colleges, college_status, college_distance, num_offers)\")\n",
    "                names.pop(i)\n",
    "                exp.pop()\n",
    "                athlete_grade.pop()\n",
    "                ages.pop()\n",
    "                ranks.pop()\n",
    "                high_school.pop()\n",
    "                home_town.pop()\n",
    "                pos_height_weight.pop()\n",
    "                colleges.pop()\n",
    "                college_status.pop()\n",
    "                college_distance.pop()\n",
    "                num_offers.pop()\n",
    "                continue\n",
    "            ###########################################################################################################################\n",
    "\n",
    "            ###########################################################################################################################\n",
    "            # Print something out to the user, like a progress bar\n",
    "            if (i % 10 == 0) and (i != 0):\n",
    "                print(\"=== Scraped {:.2f} % of the Players ===\".format((i/tot)*100))\n",
    "                print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "                      len(colleges), len(college_status), len(college_distance), len(num_offers), len(NIL_val),\n",
    "                      len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "                print(\"======================================\")\n",
    "            ###########################################################################################################################\n",
    "        \n",
    "        print(\"=== Scraped 100.0 % of the Players ===\")\n",
    "        print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "              len(colleges), len(college_status), len(college_distance), len(num_offers), len(NIL_val),\n",
    "              len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "        print(\"======================================\")\n",
    "        print('Successfully completed {} athletes for {} {}!'.format(len(names), sport, year))\n",
    "        print('Number of removed athletes: {}'.format(len(athlete_links) - len(names)))\n",
    "        #end\n",
    "    #end\n",
    "#end\n",
    "\n",
    "# Close the instance of the webpage\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n"
     ]
    }
   ],
   "source": [
    "print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "      len(colleges), len(college_status), len(college_distance), len(num_offers), len(NIL_val),\n",
    "      len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "# print(names[0], exp[0], pos_height_weight[0], athlete_grade[0], ages[0], ranks[0], high_school[0])\n",
    "# print(home_town[0], colleges[0], college_status[0], college_distance[0], num_offers[0])\n",
    "# print(NIL_val[0], instagram_followers[0], twitter_followers[0], tiktok_followers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv saved to 'csv_files' folder!'\n"
     ]
    }
   ],
   "source": [
    "column_names = ['NAME', 'EXP', 'POS_HEI_WEI', 'GRADE', 'AGE', 'SKILL', 'HISCH', 'HOTOWN', 'STARCOLL', 'STARCOLLSTAT', 'COLLDIST', 'NUMOFF', 'INSTA', 'TWIT', 'TIK', 'NILVAL']\n",
    "\n",
    "csv_file_path = 'csv_files/{}_{}.csv'.format(sport, year)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "    writer.writerow(column_names)\n",
    "\n",
    "    for row in zip(names, exp, pos_height_weight, athlete_grade, ages, ranks, high_school, home_town, colleges, college_status, college_distance, num_offers, instagram_followers, twitter_followers, tiktok_followers, NIL_val):\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"csv saved to 'csv_files' folder!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
