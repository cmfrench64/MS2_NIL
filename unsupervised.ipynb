{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "In this section we begin exploring some unsupervised learning concepts.\n",
    "\n",
    "We have two main goals from this section:\n",
    "1. Feature Importance Determination\n",
    "2. Identifiying Similiar Player Profiles of athletes with and without NIL Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Determination\n",
    "\n",
    "The outputs of this feature importance determination will be crucial in plotting and visualizing things from the KMeans Effort Below!\n",
    "\n",
    "Since our goal is to reduce our problem down for visualization purposes we are going to explore options in which the number of components is either 2 or 3 although a further analysis regarding the importance of features should also be performed.\n",
    "\n",
    "It is sometimes good for very large, high-dimensional datasets to use PCA and then apply t-SNE/UMAP after!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Components Analysis (PCA)\n",
    "\n",
    "- A deviation from PCA would be to perform information gain (IG) techniques for each column in our feature space against the ground truth label (the target). This method still suffers from information loss however.\n",
    "    - A method of creating a new feature which is linearly independent from other features. You would take k PC's which is assumed to be < # features\n",
    "    - IF YOU ARE USING PCA OR ANY METHOD TO REDUCE THE NUMBER OF FEATURES YOU MUST APPLY **AFTER** TRAIN/TEST/SPLIT TO AVOID DATA LEAKAGE!!!!!!!!!\n",
    "    - Creating a scatter plot post PCA is neat. It is also cool to look at a heatmap of the original features and the principal components\n",
    "    - BIPLOTS:\n",
    "        - combines a score plot and a loadings plot in a single graph\n",
    "        - Basically you plot all of your data on the pricipal components (lets say 2 in this case) and then you can plot the ORIGINAL features in this space! The angle between vectors in the original feature space can tell us correlation!\n",
    "    - SCREE PLOTS:\n",
    "        - Plots the eigenvalues from PCA\n",
    "        - pca.explained_variance_ from the output of the PCA var\n",
    "        - Kaiser Heursitic -- if the elbow is tough to find, keep all eigenvalues with a score of at least 1\n",
    "    - If the number of principal components is too big then you know that some kind of non-linear technique like kernel PCA might be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_Scree(X_train, n_comp=10, threshold=0.8):\n",
    "    \"\"\"\n",
    "        :: Input(s) ::\n",
    "            X_train - training data\n",
    "            n_comp - the number of components that we are going to extract with PCA\n",
    "            threshold - the amount of explained variance that the user would like to be captured by the minimum number of pricipal components\n",
    "        :: Output(s) ::\n",
    "            None\n",
    "        :: Explaination of Function ::\n",
    "        This fucntion performs Principal Components Analysis (PCA) on the training data that was provided by \"traintestsplit.ipynb\".\n",
    "        The goal here is two fold:\n",
    "            1. Generate a Scree Plot that incorporates an explained varaince ratio\n",
    "            2. Help the user determine the minumum number of components in order to consider utilizing PCA\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize the data prior to performing PCA!\n",
    "    # This is crucial as we do not want magnitude of a feature to play a role\n",
    "    X_train_Norm = StandardScaler().fit_transform(X_train)\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    X_pca = pca.fit_transform(X_train_Norm)\n",
    "\n",
    "    # Generate the Cumulative Sum of Explained Variance\n",
    "    dfCumul = pd.DataFrame({'explained_var_ratio': pca.explained_variance_ratio_,\n",
    "                            'PC': ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10']})\n",
    "    \n",
    "    dfCumul['cumulative_variance'] = dfCumul['explained_var_ratio'].cumsum()\n",
    "    threshold = threshold\n",
    "    n_components_thresh = np.argmax(dfCumul['cumulative_variance'] >= threshold) + 1\n",
    "\n",
    "    # Generate the Scree Plot\n",
    "    dfScree = pd.DataFrame({'eigenvalue': np.sqrt(pca.explained_variance_),\n",
    "                            'PC': ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10']})\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(dfScree['PC'],\n",
    "             dfScree['eigenvalue'],\n",
    "             color='skyblue')\n",
    "    plt.xlabel('Principal Component (PC)')\n",
    "    plt.ylabel('Eigenvalue')\n",
    "    plt.title('Scree Plot')\n",
    "\n",
    "    plt.axvline(dfScree['PC'][n_components_thresh - 1],\n",
    "                color='red',\n",
    "                label=\"At least {:.0f}% of Varaince Accounted for with {} PC's\".format(threshold*100, n_components_thresh))\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_BiPlot(X_train):\n",
    "    \"\"\"\n",
    "        :: Input(s) ::\n",
    "        :: Output(s) ::\n",
    "        :: Explaination of Function ::\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize the data prior to performing PCA!\n",
    "    # This is crucial as we do not want magnitude of a feature to play a role\n",
    "    X_train_Norm = StandardScaler().fit_transform(X_train)\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_train_Norm)\n",
    "\n",
    "    # The 'scores' are the individual points that make up the scatterplot in 2 dimensions\n",
    "    # X_pca\n",
    "\n",
    "    # The 'vectors' are the loadings that help map the original features onto our scatterplot\n",
    "    # The COSINE of the angle between two variable markers is the coefficent of correlation between those variables!\n",
    "    # If one of your vectors follows the x or y axis then you can correlate the variable marker and the PC!\n",
    "    # You can take the dot product of your point in 2D PC Space with each of these variable markers to \"reconstruct\" in the original feature space!\n",
    "    \n",
    "    # X = pca.components_[i,0]\n",
    "    # Y = pca.components_[i,1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multidimensional Scaling (MDS)\n",
    "\n",
    "- Distance preserving low-dimensional projection\n",
    "- Locally influenced\n",
    "- If clusters are far apart in the OG data, then they will be far apart in the MDS projection\n",
    "- weights can be introduced to handle missing data in MDS! We have lots of missing data in this project..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MDS =====\n",
    "from sklearn.manifold import MDS\n",
    "# X_norm = StandardScaler().fit_transform(X_not_norm)\n",
    "\n",
    "# mds = MDS(n_components=2)\n",
    "# X_mds = mds.fit_transform(X_norm)\n",
    "\n",
    "# plot(X_mds, output, [NAMES_OF_CLUSTERS])\n",
    "# ==============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "- Finds a low-dim output\n",
    "    - Preserves similiaries in high-dim data\n",
    "    - It is focused on preserving LOCAL DISTANCES between neighbors, not so much global structure\n",
    "    - Perplexity is the key parameter\n",
    "        - Experiment with multiple values here!\n",
    "    - Can give different output everytime it runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== tSNE =====\n",
    "from sklearn.manifold import TSNE\n",
    "# tsne = TSNE(random_state=0)\n",
    "# X_tsne = tsne.fit_transform(X_norm)\n",
    "\n",
    "# plot(X_tsne, output, [NAMES_OF_CLUSTERS])\n",
    "# ==============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform Manifold Approximation & Projection (UMAP)\n",
    "\n",
    "- Similiar to t-SNE --> Preserves local neighborhood structure\n",
    "- But it also preserves some global structure too!\n",
    "- It is a useful technique for plotting AS WELL AS clustering whereas t-SNE should not be used in clustering..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similiar Player Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes regarding KMeans clustering:\n",
    "- Different intializations can result in different solutions. Performing multiple runs is a good idea.\n",
    "    - Be careful about where you start\n",
    "    - Could place the first one randomly and the next one could be as far away as possible\n",
    "- Centroid is typically the mean of the points in the cluster.\n",
    "    - This works only when the values are continuous in nature. K-Medoid can be used if non-continuous columns are used.\n",
    "- \"Closeness\" can take the form of Euclidean Distance, Cosine Similiarity, Correlation, etc\n",
    "\n",
    "KMeans works well on simple clusters that are similiar in size, well seperated, and globular. Complex shapes... not so much...\n",
    "\n",
    "##### Visualization Tips\n",
    "\n",
    "When it comes to plotting this data it will be difficult to use normal ol KMeans because our data has lots of columns.\n",
    "- PCA would work to reduce the data down to maybe two principal components\n",
    "- t-SNE would also work to reduce the visualization down to something that is more interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must specifiy the number of clusters, K, in advance\n",
    "# We need to pick K clusters as well as the K points that will act as the initial centroid\n",
    "\n",
    "def Custom_KMeans(X_train, RANDOM_STATE=0):\n",
    "    \"\"\"\n",
    "        :: Input(s) ::\n",
    "            X_train - training data\n",
    "        :: Output(s) ::\n",
    "            A scatter plot showing the clustering\n",
    "    \"\"\"\n",
    "\n",
    "    # Remember that X_train does not contain any NIL information, that is perfectly okay because we are interested in all athletes\n",
    "    # regardless of their NIL evaluation\n",
    "\n",
    "    # We may want to normalize the features we have in some way, shape, or form...\n",
    "    X = X_train\n",
    "    X_normalized = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "    # KMeans Setup\n",
    "    clusters = 5\n",
    "    kmeans = KMeans(n_clusters=clusters,\n",
    "                    random_state=RANDOM_STATE)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # kmeans.labels_\n",
    "\n",
    "    # PCA or T-SNE Setup\n",
    "\n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
